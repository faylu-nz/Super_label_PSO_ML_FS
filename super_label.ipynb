{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "686f4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5388",
   "metadata": {},
   "source": [
    "#  Particle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8247ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    \n",
    "    def __init__(self, length, pos_max, pos_min, vel_max, vel_min, w, c1, c2, problem):\n",
    "        self.length = length\n",
    "        self.pos_max = pos_max\n",
    "        self.pos_min = pos_min\n",
    "        self.vel_max = vel_max\n",
    "        self.vel_min = vel_min\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.problem = problem\n",
    "\n",
    "        self.position = np.random.rand(length)*(pos_max-pos_min)\n",
    "        self.velocity = np.zeros(length)\n",
    "        self.fitness = self.problem.worst_fitness()\n",
    "\n",
    "        self.pbest_pos = np.zeros(length)\n",
    "        self.pbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "        self.gbest_pos = np.zeros(length)\n",
    "        self.gbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "    def update(self):\n",
    "        # Update velocity\n",
    "        self.velocity = self.w * self.velocity + \\\n",
    "            self.c1 * np.random.rand(self.length) * (self.pbest_pos - self.position) + \\\n",
    "            self.c2 * np.random.rand(self.length) * \\\n",
    "            (self.gbest_pos - self.position)\n",
    "\n",
    "        self.velocity[self.velocity < self.vel_min] = self.vel_min\n",
    "        self.velocity[self.velocity > self.vel_max] = self.vel_max\n",
    "\n",
    "        # update position\n",
    "        self.position = self.position + self.velocity\n",
    "        self.position[self.position < self.pos_min] = self.pos_min\n",
    "        self.position[self.position > self.pos_max] = self.pos_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e8f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "\n",
    "    def __init__(self, n_particles, length, pos_max, pos_min, vel_max, vel_min, problem, n_iterations):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.problem = problem\n",
    "\n",
    "        w = 0.8\n",
    "        c1 = 1.46\n",
    "        c2 = 1.46\n",
    "        self.population = [Particle(length = length, \n",
    "                                    pos_max = pos_max, pos_min = pos_min, \n",
    "                                    vel_max = vel_max, vel_min = vel_min, \n",
    "                                    w = w, c1 = c1, c2 = c2, problem = problem)\n",
    "                           for _ in range(n_particles)]\n",
    "\n",
    "    def iterate(self):\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            print('Iterate ', i, end = '  ')\n",
    "            gbest_fit = self.population[0].gbest_fit\n",
    "            gbest_index = 0\n",
    "            gbest_updated = False\n",
    "            print('gbest value is ', gbest_fit)\n",
    "            \n",
    "            for index, particle in enumerate(self.population):\n",
    "                # Evaluate each particle, update pbest\n",
    "                particle.fitness = self.problem.fitness(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.fitness, particle.pbest_fit):\n",
    "                    particle.pbest_fit = particle.fitness\n",
    "                    particle.pbest_pos = np.copy(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.pbest_fit, gbest_fit):\n",
    "                    gbest_fit = particle.pbest_fit\n",
    "                    gbest_index = index\n",
    "                    gbest_updated = True\n",
    "\n",
    "            if gbest_updated:\n",
    "                for particle in self.population:\n",
    "                    particle.gbest_fit = self.population[gbest_index].pbest_fit\n",
    "                    particle.gbest_pos = np.copy(\n",
    "                        self.population[gbest_index].pbest_pos)\n",
    "\n",
    "            # now update particle position:\n",
    "            for particle in self.population:\n",
    "                particle.update()\n",
    "\n",
    "        return self.population[0].gbest_pos, self.population[0].gbest_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330052b8",
   "metadata": {},
   "source": [
    "#  Problem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8bcdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def __init__(self, minimize):\n",
    "        self.minimize = minimize\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        return 1\n",
    "    \n",
    "    def is_better(self, first, second):\n",
    "        if self.minimize:\n",
    "            return first < second\n",
    "        else:\n",
    "            return first > second\n",
    "\n",
    "    def worst_fitness(self):\n",
    "        if self.minimize:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FS(Problem):\n",
    "\n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        clf = KNN()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_pred, y_test)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a1bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PSO\n",
    "# Fitness is MLKNN classification hamming loss.\n",
    "\n",
    "class FS_ML(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        clf = MLkNN(k=3)\n",
    "        scaler = StandardScaler()\n",
    "#         scaler = MinMaxScaler()\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1)\n",
    "\n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            ham = hamming_loss(y_test, y_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4c9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super label PSO\n",
    "# Fitness is use super+sub classification hamming loss\n",
    "\n",
    "class FS_ML_super(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1)\n",
    "            \n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "            y_test_pred, y_test = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "            \n",
    "        \n",
    "            ham = hamming_loss(y_test, y_test_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776309eb",
   "metadata": {},
   "source": [
    "# Super_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc5172b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subcgroup(cluster), get label indexes \n",
    "\n",
    "def ClusterIndicesNumpy(clustNum, labels_array): #numpy \n",
    "    return np.where(labels_array == clustNum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8ee72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subgroup's labels(binary nparray) into super label(list)\n",
    "# If all labels are 0, super label is 0; otherwise, super label is assigned to 1\n",
    "\n",
    "def convert(subgroup_label):\n",
    "    super_ = []\n",
    "    subgroup_label_array = subgroup_label.to_numpy()\n",
    "    rows = subgroup_label.shape[0]\n",
    "    columns = subgroup_label.shape[1]\n",
    "    for row in range(rows):\n",
    "        s = 0\n",
    "        for column in range(columns):\n",
    "            if subgroup_label_array[row][column] == 1:\n",
    "                s = 1\n",
    "                break\n",
    "        super_.append(s)\n",
    "    return super_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25299393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original y labels into y_super labels\n",
    "# subgroup_labels are dataframe with original labels + super labels\n",
    "\n",
    "def label_convert(y_train_, no_cls):  # Here y is dataframe\n",
    "    \n",
    "    subgroups = []\n",
    "    super_labels = []\n",
    "    kmeans = KMeans(n_clusters=no_cls, random_state=0).fit(y_train_.T)\n",
    "    dict_clst_col = dict()  # dictionary to record key(cluster index) and value(cluster columns)\n",
    "    \n",
    "    for i in range(no_cls):\n",
    "        cluster = ClusterIndicesNumpy(i, kmeans.labels_)  # Column numbers(indexes) of all the labels in each cluster\n",
    "        dict_clst_col[i] = cluster\n",
    "        subgroup_label = y_train_.iloc[:,cluster]   # Get all the original labels from cluster, dataframe form\n",
    "        s = pd.DataFrame(convert(subgroup_label), columns = ['s'+ str(i)])   # Convert original labels to a column super label\n",
    "        super_labels.append(s)\n",
    "        subgroup_label['s' + str(i)] = s   # Concat s into subgroup\n",
    "        subgroups.append(subgroup_label)        \n",
    "        \n",
    "    y_s = pd.concat(super_labels, axis=1)   # Combine all super label columns, as orginal y converted to super_label y, the target\n",
    "    y_s = y_s.to_numpy()\n",
    "\n",
    "    return y_s, subgroups, dict_clst_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eeec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split, indexes of X_train, X_test and y_train, y_test will be discorder, aka not ascending any more.\n",
    "# If index disorder, will be tricky to process index, e.g. zero_idx\n",
    "# Need to reorder index first.\n",
    "\n",
    "def convert_index(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_ = X_train.reset_index(drop=True)\n",
    "    y_train_ = y_train.reset_index(drop=True)\n",
    "    X_test_ = X_test.reset_index(drop=True)\n",
    "    y_test_ = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train_, X_test_, y_train_, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103116b4",
   "metadata": {},
   "source": [
    "When doing super and sub classification on training set, k-fold is not neccesary. \n",
    "Only the classifiers are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c970bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained super_classifier\n",
    "\n",
    "def super_classifier(X_train_, y_train_):\n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    X_train_ = StandardScaler().fit_transform(X_train_)\n",
    "#     X_train_ = MinMaxScaler().fit_transform(X_train_)\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, 2)\n",
    "    clf.fit(X_train_, y_s)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d1dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subgroup, collect all the zero super labels\n",
    "# The len of total_zeros is the number of subgroups, also the number of super labels\n",
    "\n",
    "def zeros(y_s):\n",
    "\n",
    "    total_zeros = []\n",
    "    for i in range(y_s.shape[1]):     # number of super labels\n",
    "        idx_zeros = []\n",
    "        for j in range(y_s.shape[0]):   # number of instances\n",
    "            if y_s[j][i] == 0:\n",
    "                idx_zeros.append(j)\n",
    "        total_zeros.append(idx_zeros)\n",
    "        \n",
    "    return total_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09ab36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subgroup, if a particular row of y_s is zero, the corresponding X features also needs to be removed.\n",
    "# For each subgroup, get the indexes of zeros in one y_s, and remove these same indexes from X feature instances.\n",
    "# Each subgroup contains its own X, means different subgroup contain diffenrent number of instances\n",
    "# Collect each removed X and return.\n",
    "\n",
    "def remove_zeros(X, y_s):  # y_s is ndarray\n",
    "    total_zeros = zeros(y_s)\n",
    "    Xs = []\n",
    "    for idx_zeros in total_zeros:\n",
    "        X_ = pd.DataFrame(X).drop(idx_zeros)   \n",
    "        Xs.append(X_)  \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9681",
   "metadata": {},
   "source": [
    "From original X and y, compute super label y(y_s), which actually comes from true y.\n",
    "Each subgroup contains original y labels + y_s label.\n",
    "Check each y_s, if 0, than remove the whole line, which means remove its corresponding original labels, and its X.\n",
    "So the remaining of original labels, as well as X of each subgroup are different, since indexes of zeros in each y super label are different.\n",
    "\n",
    "def sub_classification is for each subgroup, train X_(X remove y_s's zero indexes) and y_(y sub original labels remove y_s's zero indexes).\n",
    "After training, collect all sub-clfs and Xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f13ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained sub classifiers\n",
    "\n",
    "def sub_classifiers(X_train_, y_train_):\n",
    "    \n",
    "    clfs = []\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, 2)   # y_super labels, converted from original y labels (target)\n",
    "    total_zeros = zeros(y_s)\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    Xs = remove_zeros(X_train_, y_s)\n",
    "    \n",
    "    for subgroup, idx_zeros, X in zip(subgroups,total_zeros, Xs):\n",
    "                                                 # Have different X, because idx of zero are different\n",
    "        y_ = subgroup.drop(idx_zeros)            # Drop all the zero instances, both in X and y, aka X_, y_ \n",
    "        y_ = y_.drop(y_.columns[-1:], axis = 1)  # Remove the s label\n",
    "        \n",
    "        X_ = scaler.fit_transform(X)\n",
    "        clf = MLkNN(k=3)\n",
    "        clf.fit(X_, y_.to_numpy())\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    return clfs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d6bd",
   "metadata": {},
   "source": [
    "Now already gained clf, which is classifier for super classification, and clfs which are for all the sub-classifications.\n",
    "Then will apply clf and clfs on training set, to see the training_loss, and then apply on test set, to get test_loss.\n",
    "Finally, compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63521ca",
   "metadata": {},
   "source": [
    "1. Apply clf, clfs, Xs on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2d2614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do super classification on dataset\n",
    "\n",
    "def super_classification(clf_super, X_test_):\n",
    "    X_test_scaled = StandardScaler().fit_transform(X_test_)\n",
    "#     X_test_scaled = MinMaxScaler().fit_transform(X_test_)\n",
    "    y_test_s_pred = clf_super.predict(X_test_scaled).toarray()   # Predicted super labels, will be passed into def zeros().\n",
    "    return y_test_s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "759395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do sub-classification on sub-datasets (original X + subgroup original labels)\n",
    "\n",
    "def sub_classification(clfs, X_test_, y_test_s_pred):\n",
    "\n",
    "    total_test_zeros = zeros(y_test_s_pred)    # Based on predicted super label, compute which are zeros in each subgroup\n",
    "    \n",
    "    X_tests = remove_zeros(X_test_, y_test_s_pred)  # Remove zeros in each subgroup in X\n",
    "    \n",
    "    y_test_sub_preds = []\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    for clf, X_test in zip(clfs, X_tests):\n",
    "        X_scalered = scaler.fit_transform(X_test)\n",
    "        y_test_sub_pred = clf.predict(X_scalered)\n",
    "        y_test_sub_preds.append(y_test_sub_pred)\n",
    "        \n",
    "    return total_test_zeros, y_test_sub_preds    # total_test_zeros, y_test_sub_labels are lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8a5b",
   "metadata": {},
   "source": [
    "After super and sub-classifications are done on test/validation dataset, the next step is to revert and rebuild all the predicted sub-labels together. \n",
    "The predicted subgroups do not contain all the original instances, coz those all-zeros instances are removed before sub-classification. So when doing revert, we need to find out which instances are all-zeros(those predicted super-predicted are zero), these can be reverted to [0,0,0,...].\n",
    "If the super-predicted label is not 0, then this predicted instance's subgroup labels are in coreesponding y_sub_pred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdea6",
   "metadata": {},
   "source": [
    "len(total_zeros) is number of subgroups, aka number of columns for super_y_labels\n",
    "for each subgroup, y.shape[0] is the rows, aka instances in original y, \n",
    "if index of the instance is included in column in total_zeros, that means when revert to original labels, we can impute all the subgroup labels of this instance to all zeros\n",
    "\n",
    "How to impute the zero super label's corresponding sub-labels to zeros? We need to know how many sub-labels in each cluster. That is how many labels in each y_sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "382616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out zeros back into each y_sub_pred\n",
    "\n",
    "def fill_zeros(total_test_zeros, y_test_sub_preds, y_test_):\n",
    "    total_filled_preds = []\n",
    "    \n",
    "    for i in range(len(total_test_zeros)):   # number of columns of y_super\n",
    "        labels = []        \n",
    "        y_test_sub_pred_np = y_test_sub_preds[i].toarray()\n",
    "        no_sublabels = y_test_sub_pred_np.shape[1]\n",
    "        for j in range(y_test_.shape[0]):   # original y_test's rows\n",
    "            if j in total_test_zeros[i]:   \n",
    "                labels.append(np.zeros(no_sublabels, dtype=np.int64))                                          \n",
    "            else:\n",
    "                label = y_test_sub_pred_np[0]             \n",
    "                labels.append(label)\n",
    "                y_test_sub_pred_np = np.delete(y_test_sub_pred_np, [0], axis = 0)\n",
    "        total_filled_preds.append(labels)  \n",
    "        \n",
    "    return total_filled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "458fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the order of y_test_preds, aka revert to original order as y_test's columns(before clustering)\n",
    "# Clusters are not neccessarily equal, so could convert total_filled_preds to np.array\n",
    "# If cluster0 is 2, cluster is 4, will cause passed value index issuse.\n",
    "# Reference to backup_0113 version.\n",
    "\n",
    "def sort_preds(total_filled_preds, y_train_):\n",
    "    \n",
    "    y_s, subgroup, dict_clst_col = label_convert(y_train_, 2)\n",
    "    y_test_sub_preds = []\n",
    "    for i in range(len(total_filled_preds)):\n",
    "        np_total_filled_pred = np.array(total_filled_preds[i])\n",
    "        y_test_sub_preds.append(pd.DataFrame(np_total_filled_pred, columns = dict_clst_col[i]))\n",
    "    y_test_pred = pd.concat(y_test_sub_preds, axis = 1)\n",
    "    y_test_pred_t = y_test_pred.T\n",
    "    y_test_pred_t_sorted = y_test_pred_t.sort_index(ascending=True)\n",
    "    y_test_pred_sorted = y_test_pred_t_sorted.T\n",
    "\n",
    "    return y_test_pred_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be1a1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on splited X_train, y_train, calculate the y_test_pred on x_test\n",
    "\n",
    "def calc_preds(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    clf_super = super_classifier(X_train_, y_train_)\n",
    "    clfs_sub = sub_classifiers(X_train_, y_train_)\n",
    "    y_test_s_pred = super_classification(clf_super, X_test_)\n",
    "    total_test_zeros, y_test_sub_preds = sub_classification(clfs_sub, X_test_, y_test_s_pred)\n",
    "    total_filled_preds = fill_zeros(total_test_zeros, y_test_sub_preds, y_test_)\n",
    "    y_test_pred_sorted = sort_preds(total_filled_preds, y_train_)\n",
    "    \n",
    "    return y_test_pred_sorted, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611a548",
   "metadata": {},
   "source": [
    "#  Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "682bc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold only accept X, y in numpy form, and return X_train, X_test, y_train, y_test in each loop(split).\n",
    "# Convert X_train, X_test, y_train, y_test from numpy to dataframe, for super_label calculation call.\n",
    "\n",
    "def Convert_to_df(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d539a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental function.\n",
    "# To find out if cluster is not equal, what will happen.\n",
    "\n",
    "def get_not_three(X, y):\n",
    "    for i in range(5000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "        kmeans = KMeans(n_clusters=2).fit(y_train_.T)\n",
    "        cluster = ClusterIndicesNumpy(0, kmeans.labels_)\n",
    "        if len(cluster) == 3:\n",
    "            print(i, cluster)\n",
    "            return X_train_, X_test_, y_train_, y_test_, kmeans\n",
    "    print('No cluster equals to one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d091441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read arff file.\n",
    "\n",
    "def read_arff(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        header = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"@attribute\"):\n",
    "                header.append(line.split()[1])\n",
    "            elif line.startswith(\"@data\"):\n",
    "                break\n",
    "        df = pd.read_csv(f, header=None)\n",
    "        df.columns = header\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b51ff",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a48c6b",
   "metadata": {},
   "source": [
    "This version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f135172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['birds-train', 'CAL500', 'emotions', 'enron', 'flags', 'medical', 'scene', 'yeast']\n",
    "locations = [260, 68, 72, 1001, 19, 1449, 294, 103]\n",
    "n_labels = [19, 174, 6, 53, 7, 45, 6, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42eda467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10470910472386\n",
      "Iterate  2  gbest value is  0.10166311485847494\n",
      "Iterate  3  gbest value is  0.10166311485847494\n",
      "Iterate  4  gbest value is  0.10166311485847494\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.1047325435269018\n",
      "Iterate  2  gbest value is  0.10452233554469884\n",
      "Iterate  3  gbest value is  0.10154203061304254\n",
      "Iterate  4  gbest value is  0.10154203061304254\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09831199379037225\n",
      "Iterate  2  gbest value is  0.09831199379037225\n",
      "Iterate  3  gbest value is  0.09804015334541763\n",
      "Iterate  4  gbest value is  0.09804015334541763\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09906150874510802\n",
      "Iterate  2  gbest value is  0.09849433193971643\n",
      "Iterate  3  gbest value is  0.09849433193971643\n",
      "Iterate  4  gbest value is  0.09849433193971643\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10409872338665445\n",
      "Iterate  2  gbest value is  0.09669688336355002\n",
      "Iterate  3  gbest value is  0.09669688336355002\n",
      "Iterate  4  gbest value is  0.09669688336355002\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "#        problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "    problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_super_PSO_change_order_minmax.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376066d",
   "metadata": {},
   "source": [
    "#  Experiment area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38420eaa",
   "metadata": {},
   "source": [
    "Compare standard PSO FS with super PSO FS, time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10053697819836607\n",
      "Iterate  2  gbest value is  0.10053697819836607\n",
      "Iterate  3  gbest value is  0.10053697819836607\n",
      "Iterate  4  gbest value is  0.10053697819836607\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10302277941047297\n",
      "Iterate  2  gbest value is  0.10302277941047297\n",
      "Iterate  3  gbest value is  0.10302277941047297\n",
      "Iterate  4  gbest value is  0.10138353021981936\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.0962167647246712\n",
      "Iterate  2  gbest value is  0.0962167647246712\n",
      "Iterate  3  gbest value is  0.0962167647246712\n",
      "Iterate  4  gbest value is  0.0962167647246712\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10309103367896781\n",
      "Iterate  2  gbest value is  0.10309103367896781\n",
      "Iterate  3  gbest value is  0.10309103367896781\n",
      "Iterate  4  gbest value is  0.09910224523650166\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10233582798051337\n",
      "Iterate  2  gbest value is  0.10233582798051337\n",
      "Iterate  3  gbest value is  0.10233582798051337\n",
      "Iterate  4  gbest value is  0.10233582798051337\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "    problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "#     problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_standardPSO_changeorder_minmax.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21427bb",
   "metadata": {},
   "source": [
    "Compare accuracys(hamming_loss) of super classification and standard classification with full features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b49d427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "standard_hams = []\n",
    "super_hams = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get super classification acc(hl)\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "    y_test_pred_super, y_test_super = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "    super_ham = hamming_loss(y_test_super, y_test_pred_super)\n",
    "    super_hams.append(super_ham)\n",
    "    \n",
    "    # get standard classficaition acc(hl)\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    standard_ham = hamming_loss(y_test, y_test_pred)\n",
    "    standard_hams.append(standard_ham)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Standard classification with full feature hamming loss: %.4f\\n' % standard_ham\n",
    "    to_print += 'Super classification with full feature hamming loss: %.4f\\n' % super_ham\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Standard Classification Accuracy: %.4f\\n' % np.average(standard_hams)\n",
    "to_print += 'Ave Super Classification Accuracy: %.4f\\n' % np.average(super_hams)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_standard_super_clf_minmax.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
