{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686f4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from skmultilearn.dataset import available_data_sets\n",
    "from skmultilearn.dataset import load_dataset\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5388",
   "metadata": {},
   "source": [
    "#  Particle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8247ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    \n",
    "    def __init__(self, length, pos_max, pos_min, vel_max, vel_min, w, c1, c2, problem):\n",
    "        self.length = length\n",
    "        self.pos_max = pos_max\n",
    "        self.pos_min = pos_min\n",
    "        self.vel_max = vel_max\n",
    "        self.vel_min = vel_min\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.problem = problem\n",
    "\n",
    "        self.position = np.random.rand(length)*(pos_max-pos_min)\n",
    "        self.velocity = np.zeros(length)\n",
    "        self.fitness = self.problem.worst_fitness()\n",
    "\n",
    "        self.pbest_pos = np.zeros(length)\n",
    "        self.pbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "        self.gbest_pos = np.zeros(length)\n",
    "        self.gbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "    def update(self):\n",
    "        # Update velocity\n",
    "        self.velocity = self.w * self.velocity + \\\n",
    "            self.c1 * np.random.rand(self.length) * (self.pbest_pos - self.position) + \\\n",
    "            self.c2 * np.random.rand(self.length) * \\\n",
    "            (self.gbest_pos - self.position)\n",
    "\n",
    "        self.velocity[self.velocity < self.vel_min] = self.vel_min\n",
    "        self.velocity[self.velocity > self.vel_max] = self.vel_max\n",
    "\n",
    "        # update position\n",
    "        self.position = self.position + self.velocity\n",
    "        self.position[self.position < self.pos_min] = self.pos_min\n",
    "        self.position[self.position > self.pos_max] = self.pos_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "\n",
    "    def __init__(self, n_particles, length, pos_max, pos_min, vel_max, vel_min, problem, n_iterations):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.problem = problem\n",
    "\n",
    "        w = 0.8\n",
    "        c1 = 1.46\n",
    "        c2 = 1.46\n",
    "        self.population = [Particle(length = length, \n",
    "                                    pos_max = pos_max, pos_min = pos_min, \n",
    "                                    vel_max = vel_max, vel_min = vel_min, \n",
    "                                    w = w, c1 = c1, c2 = c2, problem = problem)\n",
    "                           for _ in range(n_particles)]\n",
    "\n",
    "    def iterate(self):\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            print('Iterate ', i, end = '  ')\n",
    "            gbest_fit = self.population[0].gbest_fit\n",
    "            gbest_index = 0\n",
    "            gbest_updated = False\n",
    "            print('gbest value is ', gbest_fit)\n",
    "            \n",
    "            for index, particle in enumerate(self.population):\n",
    "                # Evaluate each particle, update pbest\n",
    "                particle.fitness = self.problem.fitness(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.fitness, particle.pbest_fit):\n",
    "                    particle.pbest_fit = particle.fitness\n",
    "                    particle.pbest_pos = np.copy(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.pbest_fit, gbest_fit):\n",
    "                    gbest_fit = particle.pbest_fit\n",
    "                    gbest_index = index\n",
    "                    gbest_updated = True\n",
    "\n",
    "            if gbest_updated:\n",
    "                for particle in self.population:\n",
    "                    particle.gbest_fit = self.population[gbest_index].pbest_fit\n",
    "                    particle.gbest_pos = np.copy(\n",
    "                        self.population[gbest_index].pbest_pos)\n",
    "\n",
    "            # now update particle position:\n",
    "            for particle in self.population:\n",
    "                particle.update()\n",
    "\n",
    "        return self.population[0].gbest_pos, self.population[0].gbest_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330052b8",
   "metadata": {},
   "source": [
    "#  Problem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bcdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def __init__(self, minimize):\n",
    "        self.minimize = minimize\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        return 1\n",
    "    \n",
    "    def is_better(self, first, second):\n",
    "        if self.minimize:\n",
    "            return first < second\n",
    "        else:\n",
    "            return first > second\n",
    "\n",
    "    def worst_fitness(self):\n",
    "        if self.minimize:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FS(Problem):\n",
    "\n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        clf = KNN()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_pred, y_test)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a1bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PSO\n",
    "# Fitness is MLKNN classification hamming loss.\n",
    "\n",
    "class FS_ML(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        clf = MLkNN(k=3)\n",
    "        scaler = StandardScaler()\n",
    "#         scaler = MinMaxScaler()\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            ham = hamming_loss(y_test, y_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super label PSO\n",
    "# Fitness is use super+sub classification hamming loss\n",
    "\n",
    "class FS_ML_super(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "            \n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "            y_test_pred, y_test = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "            \n",
    "        \n",
    "            ham = hamming_loss(y_test, y_test_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776309eb",
   "metadata": {},
   "source": [
    "# Super_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5172b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subcgroup(cluster), get label indexes \n",
    "\n",
    "def ClusterIndicesNumpy(clustNum, labels_array): #numpy \n",
    "    return np.where(labels_array == clustNum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ee72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subgroup's labels(binary nparray) into super label(list)\n",
    "# If all labels are 0, super label is 0; otherwise, super label is assigned to 1\n",
    "\n",
    "def convert(subgroup_label):\n",
    "    super_ = []\n",
    "    subgroup_label_array = subgroup_label.to_numpy()\n",
    "    rows = subgroup_label.shape[0]\n",
    "    columns = subgroup_label.shape[1]\n",
    "    for row in range(rows):\n",
    "        s = 0\n",
    "        for column in range(columns):\n",
    "            if subgroup_label_array[row][column] == 1:\n",
    "                s = 1\n",
    "                break\n",
    "        super_.append(s)\n",
    "    return super_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25299393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original y labels into y_super labels\n",
    "# subgroup_labels are dataframe with original labels + super labels\n",
    "\n",
    "def label_convert(y_train_, no_cls):  # Here y is dataframe\n",
    "    \n",
    "    subgroups = []\n",
    "    super_labels = []\n",
    "    kmeans = KMeans(n_clusters=no_cls, random_state=0).fit(y_train_.T)\n",
    "    dict_clst_col = dict()  # dictionary to record key(cluster index) and value(cluster columns)\n",
    "    \n",
    "    for i in range(no_cls):\n",
    "        cluster = ClusterIndicesNumpy(i, kmeans.labels_)  # Column numbers(indexes) of all the labels in each cluster\n",
    "        dict_clst_col[i] = cluster\n",
    "        subgroup_label = y_train_.iloc[:,cluster]   # Get all the original labels from cluster, dataframe form\n",
    "        s = pd.DataFrame(convert(subgroup_label), columns = ['s'+ str(i)])   # Convert original labels to a column super label\n",
    "        super_labels.append(s)\n",
    "        subgroup_label['s' + str(i)] = s   # Concat s into subgroup\n",
    "        subgroups.append(subgroup_label)        \n",
    "        \n",
    "    y_s = pd.concat(super_labels, axis=1)   # Combine all super label columns, as orginal y converted to super_label y, the target\n",
    "    y_s = y_s.to_numpy()\n",
    "\n",
    "    return y_s, subgroups, dict_clst_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eeec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split, indexes of X_train, X_test and y_train, y_test will be discorder, aka not ascending any more.\n",
    "# If index disorder, will be tricky to process index, e.g. zero_idx\n",
    "# Need to reorder index first.\n",
    "\n",
    "def convert_index(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_ = X_train.reset_index(drop=True)\n",
    "    y_train_ = y_train.reset_index(drop=True)\n",
    "    X_test_ = X_test.reset_index(drop=True)\n",
    "    y_test_ = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train_, X_test_, y_train_, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103116b4",
   "metadata": {},
   "source": [
    "When doing super and sub classification on training set, k-fold is not neccesary. \n",
    "Only the classifiers are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c970bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained super_classifier\n",
    "\n",
    "def super_classifier(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ = scaler.fit_transform(X_train_)\n",
    "#     X_train_ = MinMaxScaler().fit_transform(X_train_)\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    clf.fit(X_train_, y_s)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subgroup, collect all the zero super labels\n",
    "# The len of total_zeros is the number of subgroups, also the number of super labels\n",
    "\n",
    "def zeros(y_s):\n",
    "\n",
    "    total_zeros = []\n",
    "    for i in range(y_s.shape[1]):     # number of super labels\n",
    "        idx_zeros = []\n",
    "        for j in range(y_s.shape[0]):   # number of instances\n",
    "            if y_s[j][i] == 0:\n",
    "                idx_zeros.append(j)\n",
    "        total_zeros.append(idx_zeros)\n",
    "        \n",
    "    return total_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ab36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subgroup, if a particular row of y_s is zero, the corresponding X features also needs to be removed.\n",
    "# For each subgroup, get the indexes of zeros in one y_s, and remove these same indexes from X feature instances.\n",
    "# Each subgroup contains its own X, means different subgroup contain diffenrent number of instances\n",
    "# Collect each removed X and return.\n",
    "\n",
    "def remove_zeros(X, y_s):  # y_s is ndarray\n",
    "    total_zeros = zeros(y_s)\n",
    "    Xs = []\n",
    "    for idx_zeros in total_zeros:\n",
    "        X_ = pd.DataFrame(X).drop(idx_zeros)   \n",
    "        Xs.append(X_)  \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9681",
   "metadata": {},
   "source": [
    "From original X and y, compute super label y(y_s), which actually comes from true y.\n",
    "Each subgroup contains original y labels + y_s label.\n",
    "Check each y_s, if 0, than remove the whole line, which means remove its corresponding original labels, and its X.\n",
    "So the remaining of original labels, as well as X of each subgroup are different, since indexes of zeros in each y super label are different.\n",
    "\n",
    "def sub_classification is for each subgroup, train X_(X remove y_s's zero indexes) and y_(y sub original labels remove y_s's zero indexes).\n",
    "After training, collect all sub-clfs and Xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained sub classifiers\n",
    "\n",
    "def sub_classifiers(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clfs = []\n",
    "    n_sub_labelses = []\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)   # y_super labels, converted from original y labels (target)\n",
    "    total_zeros = zeros(y_s)\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    Xs = remove_zeros(X_train_, y_s)\n",
    "    \n",
    "    for subgroup, idx_zeros, X in zip(subgroups,total_zeros, Xs):\n",
    "                                                 # Have different X, because idx of zero are different\n",
    "        y_ = subgroup.drop(idx_zeros)            # Drop all the zero instances, both in X and y, aka X_, y_ \n",
    "        y_ = y_.drop(y_.columns[-1:], axis = 1)  # Remove the s label\n",
    "        n_sub_labels = subgroup.shape[1]-1\n",
    "        n_sub_labelses.append(n_sub_labels)\n",
    "        \n",
    "        X_ = scaler.fit_transform(X)\n",
    "        clf = MLkNN(k=3)\n",
    "        clf.fit(X_, y_.to_numpy())\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    return clfs, n_sub_labelses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d6bd",
   "metadata": {},
   "source": [
    "Now already gained clf, which is classifier for super classification, and clfs which are for all the sub-classifications.\n",
    "Then will apply clf and clfs on training set, to see the training_loss, and then apply on test set, to get test_loss.\n",
    "Finally, compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63521ca",
   "metadata": {},
   "source": [
    "1. Apply clf, clfs, Xs on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d2614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do super classification on dataset\n",
    "\n",
    "def super_classification(clf_super, X_test_):\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test_)\n",
    "#     X_test_scaled = MinMaxScaler().fit_transform(X_test_)\n",
    "    y_test_s_pred = clf_super.predict(X_test_scaled).toarray()   # Predicted super labels, will be passed into def zeros().\n",
    "  \n",
    "    return y_test_s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "759395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do sub-classification on sub-datasets (original X + subgroup original labels)\n",
    "\n",
    "def sub_classification(clfs, n_sub_labelses, X_test_, y_test_s_pred):\n",
    "\n",
    "    total_test_zeros = zeros(y_test_s_pred)    # Based on predicted super label, compute which are zeros in each subgroup\n",
    "    \n",
    "    X_tests = remove_zeros(X_test_, y_test_s_pred)  # Remove zeros in each subgroup in X\n",
    "    \n",
    "    y_test_sub_preds = []\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    for clf, X_test in zip(clfs, X_tests):\n",
    "        if len(X_test.axes[0]) == 0:  # based on corrrespanding y_s_pred column, the y_labels in column are all zeros\n",
    "            y_test_sub_pred = None  # so when remove zeros in X_test, n_rowsof X-test is 0, aka no intances to predict\n",
    "            y_test_sub_preds.append(y_test_sub_pred)\n",
    "        else:\n",
    "            X_scaled = scaler.fit_transform(X_test)\n",
    "            y_test_sub_pred = clf.predict(X_scaled)   # y_test_sub_pred is sparse matrix\n",
    "            y_test_sub_preds.append(y_test_sub_pred)\n",
    "        \n",
    "    return total_test_zeros, y_test_sub_preds    # total_test_zeros, y_test_sub_labels are lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8a5b",
   "metadata": {},
   "source": [
    "After super and sub-classifications are done on test/validation dataset, the next step is to revert and rebuild all the predicted sub-labels together. \n",
    "The predicted subgroups do not contain all the original instances, coz those all-zeros instances are removed before sub-classification. So when doing revert, we need to find out which instances are all-zeros(those predicted super-predicted are zero), these can be reverted to [0,0,0,...].\n",
    "If the super-predicted label is not 0, then this predicted instance's subgroup labels are in coreesponding y_sub_pred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdea6",
   "metadata": {},
   "source": [
    "len(total_zeros) is number of subgroups, aka number of columns for super_y_labels\n",
    "for each subgroup, y.shape[0] is the rows, aka instances in original y, \n",
    "if index of the instance is included in column in total_zeros, that means when revert to original labels, we can impute all the subgroup labels of this instance to all zeros\n",
    "\n",
    "How to impute the zero super label's corresponding sub-labels to zeros? We need to know how many sub-labels in each cluster. That is how many labels in each y_sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba26e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out zeros back into each y_sub_pred\n",
    "\n",
    "def fill_zeros(total_test_zeros, y_test_sub_preds, y_test_, n_sub_labelses):\n",
    "    total_filled_preds = []\n",
    "    \n",
    "    for i in range(len(total_test_zeros)):   # number of columns of y_super\n",
    "        labels = []     \n",
    "        if y_test_sub_preds[i] == None:\n",
    "            for j in range(y_test_.shape[0]):   \n",
    "                labels.append(np.zeros(n_sub_labelses[i], dtype=np.int64))   # Create all zero list, then list of list, append\n",
    "        else:\n",
    "            y_test_sub_pred_np = y_test_sub_preds[i].toarray()     # y_test_sub_pred is sparse matrix, convert to nparray\n",
    "            no_sublabels = y_test_sub_pred_np.shape[1]\n",
    "            for j in range(y_test_.shape[0]):   # original y_test's rows\n",
    "                if j in total_test_zeros[i]:   \n",
    "                    labels.append(np.zeros(no_sublabels, dtype=np.int64))                                          \n",
    "                else:\n",
    "                    label = y_test_sub_pred_np[0]             \n",
    "                    labels.append(label)\n",
    "                    y_test_sub_pred_np = np.delete(y_test_sub_pred_np, [0], axis = 0)\n",
    "        total_filled_preds.append(labels)  \n",
    "        \n",
    "    return total_filled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the order of y_test_preds, aka revert to original order as y_test's columns(before clustering)\n",
    "# Clusters are not neccessarily equal, so could convert total_filled_preds to np.array\n",
    "\n",
    "def sort_preds(total_filled_preds, y_train_, no_cls):\n",
    "    \n",
    "    y_s, subgroup, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    y_test_sub_preds = []\n",
    "    for i in range(len(total_filled_preds)):\n",
    "        np_total_filled_pred = np.array(total_filled_preds[i])\n",
    "        y_test_sub_preds.append(pd.DataFrame(np_total_filled_pred, columns = dict_clst_col[i]))\n",
    "    y_test_pred = pd.concat(y_test_sub_preds, axis = 1)\n",
    "    y_test_pred_t = y_test_pred.T\n",
    "    y_test_pred_t_sorted = y_test_pred_t.sort_index(ascending=True)\n",
    "    y_test_pred_sorted = y_test_pred_t_sorted.T\n",
    "\n",
    "    return y_test_pred_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1a1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on splited X_train, y_train, calculate the y_test_pred on x_test\n",
    "\n",
    "def calc_preds(X_train, y_train, X_test, y_test, no_cls):\n",
    "    \n",
    "    X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    clf_super = super_classifier(X_train_, y_train_, no_cls)\n",
    "    clfs_sub, n_sub_labelses = sub_classifiers(X_train_, y_train_, no_cls)\n",
    "    y_test_s_pred = super_classification(clf_super, X_test_)\n",
    "    total_test_zeros, y_test_sub_preds = sub_classification(clfs_sub, n_sub_labelses, X_test_, y_test_s_pred)\n",
    "    total_filled_preds = fill_zeros(total_test_zeros, y_test_sub_preds, y_test_, n_sub_labelses)\n",
    "    y_test_pred_sorted = sort_preds(total_filled_preds, y_train_, no_cls)\n",
    "    \n",
    "    return y_test_pred_sorted, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611a548",
   "metadata": {},
   "source": [
    "#  Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682bc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold only accept X, y in numpy form, and return X_train, X_test, y_train, y_test in each loop(split).\n",
    "# Convert X_train, X_test, y_train, y_test from numpy to dataframe, for super_label calculation call.\n",
    "\n",
    "def Convert_to_df(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d091441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read arff file.\n",
    "\n",
    "def read_arff(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        header = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"@attribute\"):\n",
    "                header.append(line.split()[1])\n",
    "            elif line.startswith(\"@data\"):\n",
    "                break\n",
    "        df = pd.read_csv(f, header=None)\n",
    "        df.columns = header\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b51ff",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5358fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corel5k',\n",
       " 'bibtex',\n",
       " 'birds',\n",
       " 'delicious',\n",
       " 'emotions',\n",
       " 'enron',\n",
       " 'genbase',\n",
       " 'mediamill',\n",
       " 'medical',\n",
       " 'rcv1subset1',\n",
       " 'rcv1subset2',\n",
       " 'rcv1subset3',\n",
       " 'rcv1subset4',\n",
       " 'rcv1subset5',\n",
       " 'scene',\n",
       " 'tmc2007_500',\n",
       " 'yeast'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in available_data_sets().keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f135172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['CAL500', 'flags']\n",
    "n_featureses = [68, 19]\n",
    "n_labels = [174, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2615062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_small = ['emotions', 'scene']\n",
    "datasets_medium = ['yeast', 'birds', 'genbase']\n",
    "datasets_large = ['medical', 'enron', 'bibtex', 'Corel5k']\n",
    "\n",
    "no_clses_small = [2, 4]\n",
    "no_clses_medium = [2, 4, 6]\n",
    "no_clses_large = [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea597da6",
   "metadata": {},
   "source": [
    "Standard classification with full features vs Super classification with full features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f170455",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 488. MiB for an array with shape (8000, 8000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-8e81b667e173>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mno_cls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mno_clses_small\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mX_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvert_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0my_test_pred_super\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_super\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_preds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0msuper_ham\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_super\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred_super\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mdict_cls_ham\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_cls\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper_ham\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-2acbefb8de8a>\u001b[0m in \u001b[0;36mcalc_preds\u001b[1;34m(X_train, y_train, X_test, y_test, no_cls)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclf_super\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mclfs_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_sub_labelses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_classifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my_test_s_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_super\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7a5a688225af>\u001b[0m in \u001b[0;36msuper_classifier\u001b[1;34m(X_train_, y_train_, no_cls)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     X_train_ = MinMaxScaler().fit_transform(X_train_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_clst_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skmultilearn\\adapt\\mlknn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prior_prob_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prior_prob_false\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Computing the posterior probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond_prob_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond_prob_false\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skmultilearn\\adapt\\mlknn.py\u001b[0m in \u001b[0;36m_compute_cond\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         neighbors = [a[self.ignore_first_neighbours:] for a in\n\u001b[1;32m--> 172\u001b[1;33m                      self.knn_.kneighbors(X, self.k + self.ignore_first_neighbours, return_distance=False)]\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_instances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             chunked_results = list(pairwise_distances_chunked(\n\u001b[0m\u001b[0;32m    706\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1623\u001b[1;33m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[0;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[0;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 488. MiB for an array with shape (8000, 8000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "# for dataset in datasets_small:\n",
    "for dataset, n_features in zip(datasets, n_featureses):\n",
    "    \n",
    "    data = read_arff('datasets/' + dataset + '.arff')\n",
    "\n",
    "    y = data.iloc[:10000, n_features:]\n",
    "    X = data.iloc[:10000, :n_features]\n",
    "\n",
    "#     n_features = len(list(X))\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "#     X, y, feature_names, label_names = load_dataset(dataset, 'undivided')\n",
    "#     X = pd.DataFrame.sparse.from_spmatrix(X).to_numpy()\n",
    "#     y = pd.DataFrame.sparse.from_spmatrix(y).to_numpy()\n",
    "\n",
    "    n_splits = 5\n",
    "    k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "    standard_hams = []\n",
    "    super_hams = []\n",
    "    to_print = ''\n",
    "    fold_count = 0\n",
    "\n",
    "    for train_idx, test_idx in k_fold.split(X, y):\n",
    "        fold_count += 1\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "        # get super classification acc(hl)\n",
    "        dict_cls_ham = dict()   # no_cls:super_ham dictionary\n",
    "        for no_cls in no_clses_small:        \n",
    "            X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "            y_test_pred_super, y_test_super = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df, no_cls)\n",
    "            super_ham = hamming_loss(y_test_super, y_test_pred_super)        \n",
    "            dict_cls_ham[no_cls] = super_ham\n",
    "            super_hams.append(dict_cls_ham)\n",
    "\n",
    "\n",
    "        # get standard classficaition acc(hl)\n",
    "        scaler = StandardScaler()\n",
    "    #     scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "        clf = MLkNN(k=3)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_test_pred = clf.predict(X_test_scaled)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        standard_ham = hamming_loss(y_test, y_test_pred)\n",
    "        standard_hams.append(standard_ham)\n",
    "\n",
    "        # to write the results\n",
    "        to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "        to_print += 'Standard classification with full feature hamming loss: %.4f\\n' % standard_ham\n",
    "        to_print += 'Super classification with full feature hamming loss with 2 clusters: %.4f\\n' % dict_cls_ham[2]\n",
    "        to_print += 'Super classification with full feature hamming loss with 4 clusters: %.4f\\n' % dict_cls_ham[4]\n",
    "        to_print += 'Super classification with full feature hamming loss with 6 clusters: %.4f\\n' % dict_cls_ham[6]\n",
    "        to_print += 'Super classification with full feature hamming loss with 8 clusters: %.4f\\n' % dict_cls_ham[8]\n",
    "        to_print += 'Super classification with full feature hamming loss with 10 clusters: %.4f\\n' % dict_cls_ham[10]\n",
    "\n",
    "    to_print += '--------------Average----------------\\n'\n",
    "    to_print += 'Ave Standard Classification Accuracy: %.4f\\n' % np.average(standard_hams)\n",
    "    # to_print += 'Ave Super Classification Accuracy: %.4f\\n' % np.average(super_hams)\n",
    "    to_print += 'Ave Super Classification with 2 clusters: %.4f\\n' % np.average([super_hams[0][2], super_hams[1][2], super_hams[2][2], super_hams[3][2], super_hams[4][2]])\n",
    "    to_print += 'Ave Super Classification with 4 clusters: %.4f\\n' % np.average([super_hams[0][4], super_hams[1][4], super_hams[2][4], super_hams[3][4], super_hams[4][4]])\n",
    "    to_print += 'Ave Super Classification with 6 clusters: %.4f\\n' % np.average([super_hams[0][6], super_hams[1][6], super_hams[2][6], super_hams[3][6], super_hams[4][6]])\n",
    "    to_print += 'Ave Super Classification with 8 clusters: %.4f\\n' % np.average([super_hams[0][8], super_hams[1][8], super_hams[2][8], super_hams[3][8], super_hams[4][8]])\n",
    "    to_print += 'Ave Super Classification with 10 clusters: %.4f\\n' % np.average([super_hams[0][10], super_hams[1][10], super_hams[2][10], super_hams[3][10], super_hams[4][10]])\n",
    "\n",
    "    f = open('records/record_stdscaler_' + dataset + '_full_standard_super_clf.txt', 'w')\n",
    "    f.write(to_print)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702a97f",
   "metadata": {},
   "source": [
    "Full feature standard classfification acc vs Super classification PSO selected acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42eda467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10470910472386\n",
      "Iterate  2  gbest value is  0.10166311485847494\n",
      "Iterate  3  gbest value is  0.10166311485847494\n",
      "Iterate  4  gbest value is  0.10166311485847494\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.1047325435269018\n",
      "Iterate  2  gbest value is  0.10452233554469884\n",
      "Iterate  3  gbest value is  0.10154203061304254\n",
      "Iterate  4  gbest value is  0.10154203061304254\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09831199379037225\n",
      "Iterate  2  gbest value is  0.09831199379037225\n",
      "Iterate  3  gbest value is  0.09804015334541763\n",
      "Iterate  4  gbest value is  0.09804015334541763\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09906150874510802\n",
      "Iterate  2  gbest value is  0.09849433193971643\n",
      "Iterate  3  gbest value is  0.09849433193971643\n",
      "Iterate  4  gbest value is  0.09849433193971643\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10409872338665445\n",
      "Iterate  2  gbest value is  0.09669688336355002\n",
      "Iterate  3  gbest value is  0.09669688336355002\n",
      "Iterate  4  gbest value is  0.09669688336355002\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "# y = data.iloc[:, locations[6]:]\n",
    "# X = data.iloc[:, :locations[6]]\n",
    "# n_features = len(list(X))\n",
    "# X = X.to_numpy()\n",
    "# y = y.to_numpy()\n",
    "\n",
    "X, y, feature_names, label_names = load_dataset(datasets_large[0], 'undivided')\n",
    "X = pd.DataFrame.sparse.from_spmatrix(X).to_numpy()\n",
    "y = pd.DataFrame.sparse.from_spmatrix(y).to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "#        problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "    problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_super_PSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38420eaa",
   "metadata": {},
   "source": [
    "Standard PSO FS time cost vs Super PSO FS time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10053697819836607\n",
      "Iterate  2  gbest value is  0.10053697819836607\n",
      "Iterate  3  gbest value is  0.10053697819836607\n",
      "Iterate  4  gbest value is  0.10053697819836607\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10302277941047297\n",
      "Iterate  2  gbest value is  0.10302277941047297\n",
      "Iterate  3  gbest value is  0.10302277941047297\n",
      "Iterate  4  gbest value is  0.10138353021981936\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.0962167647246712\n",
      "Iterate  2  gbest value is  0.0962167647246712\n",
      "Iterate  3  gbest value is  0.0962167647246712\n",
      "Iterate  4  gbest value is  0.0962167647246712\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10309103367896781\n",
      "Iterate  2  gbest value is  0.10309103367896781\n",
      "Iterate  3  gbest value is  0.10309103367896781\n",
      "Iterate  4  gbest value is  0.09910224523650166\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10233582798051337\n",
      "Iterate  2  gbest value is  0.10233582798051337\n",
      "Iterate  3  gbest value is  0.10233582798051337\n",
      "Iterate  4  gbest value is  0.10233582798051337\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "    problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "#     problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_standardPSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
