{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686f4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from skmultilearn.dataset import available_data_sets\n",
    "from skmultilearn.dataset import load_dataset\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5388",
   "metadata": {},
   "source": [
    "#  Particle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8247ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    \n",
    "    def __init__(self, length, pos_max, pos_min, vel_max, vel_min, w, c1, c2, problem):\n",
    "        self.length = length\n",
    "        self.pos_max = pos_max\n",
    "        self.pos_min = pos_min\n",
    "        self.vel_max = vel_max\n",
    "        self.vel_min = vel_min\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.problem = problem\n",
    "\n",
    "        self.position = np.random.rand(length)*(pos_max-pos_min)\n",
    "        self.velocity = np.zeros(length)\n",
    "        self.fitness = self.problem.worst_fitness()\n",
    "\n",
    "        self.pbest_pos = np.zeros(length)\n",
    "        self.pbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "        self.gbest_pos = np.zeros(length)\n",
    "        self.gbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "    def update(self):\n",
    "        # Update velocity\n",
    "        self.velocity = self.w * self.velocity + \\\n",
    "            self.c1 * np.random.rand(self.length) * (self.pbest_pos - self.position) + \\\n",
    "            self.c2 * np.random.rand(self.length) * \\\n",
    "            (self.gbest_pos - self.position)\n",
    "\n",
    "        self.velocity[self.velocity < self.vel_min] = self.vel_min\n",
    "        self.velocity[self.velocity > self.vel_max] = self.vel_max\n",
    "\n",
    "        # update position\n",
    "        self.position = self.position + self.velocity\n",
    "        self.position[self.position < self.pos_min] = self.pos_min\n",
    "        self.position[self.position > self.pos_max] = self.pos_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "\n",
    "    def __init__(self, n_particles, length, pos_max, pos_min, vel_max, vel_min, problem, n_iterations):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.problem = problem\n",
    "\n",
    "        w = 0.8\n",
    "        c1 = 1.46\n",
    "        c2 = 1.46\n",
    "        self.population = [Particle(length = length, \n",
    "                                    pos_max = pos_max, pos_min = pos_min, \n",
    "                                    vel_max = vel_max, vel_min = vel_min, \n",
    "                                    w = w, c1 = c1, c2 = c2, problem = problem)\n",
    "                           for _ in range(n_particles)]\n",
    "\n",
    "    def iterate(self):\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            print('Iterate ', i, end = '  ')\n",
    "            gbest_fit = self.population[0].gbest_fit\n",
    "            gbest_index = 0\n",
    "            gbest_updated = False\n",
    "            print('gbest value is ', gbest_fit)\n",
    "            \n",
    "            for index, particle in enumerate(self.population):\n",
    "                # Evaluate each particle, update pbest\n",
    "                particle.fitness = self.problem.fitness(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.fitness, particle.pbest_fit):\n",
    "                    particle.pbest_fit = particle.fitness\n",
    "                    particle.pbest_pos = np.copy(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.pbest_fit, gbest_fit):\n",
    "                    gbest_fit = particle.pbest_fit\n",
    "                    gbest_index = index\n",
    "                    gbest_updated = True\n",
    "\n",
    "            if gbest_updated:\n",
    "                for particle in self.population:\n",
    "                    particle.gbest_fit = self.population[gbest_index].pbest_fit\n",
    "                    particle.gbest_pos = np.copy(\n",
    "                        self.population[gbest_index].pbest_pos)\n",
    "\n",
    "            # now update particle position:\n",
    "            for particle in self.population:\n",
    "                particle.update()\n",
    "\n",
    "        return self.population[0].gbest_pos, self.population[0].gbest_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330052b8",
   "metadata": {},
   "source": [
    "#  Problem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bcdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def __init__(self, minimize):\n",
    "        self.minimize = minimize\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        return 1\n",
    "    \n",
    "    def is_better(self, first, second):\n",
    "        if self.minimize:\n",
    "            return first < second\n",
    "        else:\n",
    "            return first > second\n",
    "\n",
    "    def worst_fitness(self):\n",
    "        if self.minimize:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FS(Problem):\n",
    "\n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        clf = KNN()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_pred, y_test)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a1bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PSO\n",
    "# Fitness is MLKNN classification hamming loss.\n",
    "\n",
    "class FS_ML(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        clf = MLkNN(k=3)\n",
    "        scaler = StandardScaler()\n",
    "#         scaler = MinMaxScaler()\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            ham = hamming_loss(y_test, y_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super label PSO\n",
    "# Fitness is use super+sub classification hamming loss\n",
    "\n",
    "class FS_ML_super(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "            \n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "            y_test_pred, y_test = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "            \n",
    "        \n",
    "            ham = hamming_loss(y_test, y_test_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776309eb",
   "metadata": {},
   "source": [
    "# Super_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5172b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subcgroup(cluster), get label indexes \n",
    "\n",
    "def ClusterIndicesNumpy(clustNum, labels_array): #numpy \n",
    "    return np.where(labels_array == clustNum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ee72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subgroup's labels(binary nparray) into super label(list)\n",
    "# If all labels are 0, super label is 0; otherwise, super label is assigned to 1\n",
    "\n",
    "def convert(subgroup_label):\n",
    "    super_ = []\n",
    "    subgroup_label_array = subgroup_label.to_numpy()\n",
    "    rows = subgroup_label.shape[0]\n",
    "    columns = subgroup_label.shape[1]\n",
    "    for row in range(rows):\n",
    "        s = 0\n",
    "        for column in range(columns):\n",
    "            if subgroup_label_array[row][column] == 1:\n",
    "                s = 1\n",
    "                break\n",
    "        super_.append(s)\n",
    "    return super_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25299393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original y labels into y_super labels\n",
    "# subgroup_labels are dataframe with original labels + super labels\n",
    "\n",
    "def label_convert(y_train_, no_cls):  # Here y is dataframe\n",
    "    \n",
    "    subgroups = []\n",
    "    super_labels = []\n",
    "    kmeans = KMeans(n_clusters=no_cls, random_state=0).fit(y_train_.T)\n",
    "    dict_clst_col = dict()  # dictionary to record key(cluster index) and value(cluster columns)\n",
    "    \n",
    "    for i in range(no_cls):\n",
    "        cluster = ClusterIndicesNumpy(i, kmeans.labels_)  # Column numbers(indexes) of all the labels in each cluster\n",
    "        dict_clst_col[i] = cluster\n",
    "        subgroup_label = y_train_.iloc[:,cluster]   # Get all the original labels from cluster, dataframe form\n",
    "        s = pd.DataFrame(convert(subgroup_label), columns = ['s'+ str(i)])   # Convert original labels to a column super label\n",
    "        super_labels.append(s)\n",
    "        subgroup_label['s' + str(i)] = s   # Concat s into subgroup\n",
    "        subgroups.append(subgroup_label)        \n",
    "        \n",
    "    y_s = pd.concat(super_labels, axis=1)   # Combine all super label columns, as orginal y converted to super_label y, the target\n",
    "    y_s = y_s.to_numpy()\n",
    "\n",
    "    return y_s, subgroups, dict_clst_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eeec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split, indexes of X_train, X_test and y_train, y_test will be discorder, aka not ascending any more.\n",
    "# If index disorder, will be tricky to process index, e.g. zero_idx\n",
    "# Need to reorder index first.\n",
    "\n",
    "def convert_index(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_ = X_train.reset_index(drop=True)\n",
    "    y_train_ = y_train.reset_index(drop=True)\n",
    "    X_test_ = X_test.reset_index(drop=True)\n",
    "    y_test_ = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train_, X_test_, y_train_, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103116b4",
   "metadata": {},
   "source": [
    "When doing super and sub classification on training set, k-fold is not neccesary. \n",
    "Only the classifiers are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c970bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained super_classifier\n",
    "\n",
    "def super_classifier(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ = scaler.fit_transform(X_train_)\n",
    "#     X_train_ = MinMaxScaler().fit_transform(X_train_)\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    clf.fit(X_train_, y_s)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subgroup, collect all the zero super labels\n",
    "# The len of total_zeros is the number of subgroups, also the number of super labels\n",
    "\n",
    "def zeros(y_s):\n",
    "\n",
    "    total_zeros = []\n",
    "    for i in range(y_s.shape[1]):     # number of super labels\n",
    "        idx_zeros = []\n",
    "        for j in range(y_s.shape[0]):   # number of instances\n",
    "            if y_s[j][i] == 0:\n",
    "                idx_zeros.append(j)\n",
    "        total_zeros.append(idx_zeros)\n",
    "        \n",
    "    return total_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ab36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subgroup, if a particular row of y_s is zero, the corresponding X features also needs to be removed.\n",
    "# For each subgroup, get the indexes of zeros in one y_s, and remove these same indexes from X feature instances.\n",
    "# Each subgroup contains its own X, means different subgroup contain diffenrent number of instances\n",
    "# Collect each removed X and return.\n",
    "\n",
    "def remove_zeros(X, y_s):  # y_s is ndarray\n",
    "    total_zeros = zeros(y_s)\n",
    "    Xs = []\n",
    "    for idx_zeros in total_zeros:\n",
    "        X_ = pd.DataFrame(X).drop(idx_zeros)   \n",
    "        Xs.append(X_)  \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9681",
   "metadata": {},
   "source": [
    "From original X and y, compute super label y(y_s), which actually comes from true y.\n",
    "Each subgroup contains original y labels + y_s label.\n",
    "Check each y_s, if 0, than remove the whole line, which means remove its corresponding original labels, and its X.\n",
    "So the remaining of original labels, as well as X of each subgroup are different, since indexes of zeros in each y super label are different.\n",
    "\n",
    "def sub_classification is for each subgroup, train X_(X remove y_s's zero indexes) and y_(y sub original labels remove y_s's zero indexes).\n",
    "After training, collect all sub-clfs and Xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained sub classifiers\n",
    "\n",
    "def sub_classifiers(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clfs = []\n",
    "    n_sub_labelses = []\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)   # y_super labels, converted from original y labels (target)\n",
    "    total_zeros = zeros(y_s)\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    Xs = remove_zeros(X_train_, y_s)\n",
    "    \n",
    "    for subgroup, idx_zeros, X in zip(subgroups,total_zeros, Xs):\n",
    "                                                 # Have different X, because idx of zero are different\n",
    "        y_ = subgroup.drop(idx_zeros)            # Drop all the zero instances, both in X and y, aka X_, y_ \n",
    "        y_ = y_.drop(y_.columns[-1:], axis = 1)  # Remove the s label\n",
    "        n_sub_labels = subgroup.shape[1]-1\n",
    "        n_sub_labelses.append(n_sub_labels)\n",
    "        \n",
    "        X_ = scaler.fit_transform(X)\n",
    "        clf = MLkNN(k=3)\n",
    "        clf.fit(X_, y_.to_numpy())\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    return clfs, n_sub_labelses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d6bd",
   "metadata": {},
   "source": [
    "Now already gained clf, which is classifier for super classification, and clfs which are for all the sub-classifications.\n",
    "Then will apply clf and clfs on training set, to see the training_loss, and then apply on test set, to get test_loss.\n",
    "Finally, compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63521ca",
   "metadata": {},
   "source": [
    "1. Apply clf, clfs, Xs on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d2614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do super classification on dataset\n",
    "\n",
    "def super_classification(clf_super, X_test_):\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test_)\n",
    "#     X_test_scaled = MinMaxScaler().fit_transform(X_test_)\n",
    "    y_test_s_pred = clf_super.predict(X_test_scaled).toarray()   # Predicted super labels, will be passed into def zeros().\n",
    "  \n",
    "    return y_test_s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "759395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do sub-classification on sub-datasets (original X + subgroup original labels)\n",
    "\n",
    "def sub_classification(clfs, n_sub_labelses, X_test_, y_test_s_pred):\n",
    "\n",
    "    total_test_zeros = zeros(y_test_s_pred)    # Based on predicted super label, compute which are zeros in each subgroup\n",
    "    \n",
    "    X_tests = remove_zeros(X_test_, y_test_s_pred)  # Remove zeros in each subgroup in X\n",
    "    \n",
    "    y_test_sub_preds = []\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    for clf, X_test in zip(clfs, X_tests):\n",
    "        if len(X_test.axes[0]) == 0:  # based on corrrespanding y_s_pred column, the y_labels in column are all zeros\n",
    "            y_test_sub_pred = None  # so when remove zeros in X_test, n_rowsof X-test is 0, aka no intances to predict\n",
    "            y_test_sub_preds.append(y_test_sub_pred)\n",
    "        else:\n",
    "            X_scaled = scaler.fit_transform(X_test)\n",
    "            y_test_sub_pred = clf.predict(X_scaled)   # y_test_sub_pred is sparse matrix\n",
    "            y_test_sub_preds.append(y_test_sub_pred)\n",
    "        \n",
    "    return total_test_zeros, y_test_sub_preds    # total_test_zeros, y_test_sub_labels are lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8a5b",
   "metadata": {},
   "source": [
    "After super and sub-classifications are done on test/validation dataset, the next step is to revert and rebuild all the predicted sub-labels together. \n",
    "The predicted subgroups do not contain all the original instances, coz those all-zeros instances are removed before sub-classification. So when doing revert, we need to find out which instances are all-zeros(those predicted super-predicted are zero), these can be reverted to [0,0,0,...].\n",
    "If the super-predicted label is not 0, then this predicted instance's subgroup labels are in coreesponding y_sub_pred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdea6",
   "metadata": {},
   "source": [
    "len(total_zeros) is number of subgroups, aka number of columns for super_y_labels\n",
    "for each subgroup, y.shape[0] is the rows, aka instances in original y, \n",
    "if index of the instance is included in column in total_zeros, that means when revert to original labels, we can impute all the subgroup labels of this instance to all zeros\n",
    "\n",
    "How to impute the zero super label's corresponding sub-labels to zeros? We need to know how many sub-labels in each cluster. That is how many labels in each y_sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out zeros back into each y_sub_pred\n",
    "\n",
    "def fill_zeros(total_test_zeros, y_test_sub_preds, y_test_, n_sub_labelses):\n",
    "    total_filled_preds = []\n",
    "    \n",
    "    for i in range(len(total_test_zeros)):   # number of columns of y_super\n",
    "        labels = []     \n",
    "        if y_test_sub_preds[i] == None:\n",
    "            for j in range(y_test_.shape[0]):   \n",
    "                labels.append(np.zeros(n_sub_labelses[i], dtype=np.int64))   # Create all zero list, then list of list, append\n",
    "        else:\n",
    "            y_test_sub_pred_np = y_test_sub_preds[i].toarray()     # y_test_sub_pred is sparse matrix, convert to nparray\n",
    "            no_sublabels = y_test_sub_pred_np.shape[1]\n",
    "            for j in range(y_test_.shape[0]):   # original y_test's rows\n",
    "                if j in total_test_zeros[i]:   \n",
    "                    labels.append(np.zeros(no_sublabels, dtype=np.int64))                                          \n",
    "                else:\n",
    "                    label = y_test_sub_pred_np[0]             \n",
    "                    labels.append(label)\n",
    "                    y_test_sub_pred_np = np.delete(y_test_sub_pred_np, [0], axis = 0)\n",
    "        total_filled_preds.append(labels)  \n",
    "        \n",
    "    return total_filled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the order of y_test_preds, aka revert to original order as y_test's columns(before clustering)\n",
    "# Clusters are not neccessarily equal, so could convert total_filled_preds to np.array\n",
    "\n",
    "def sort_preds(total_filled_preds, y_train_, no_cls):\n",
    "    \n",
    "    y_s, subgroup, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    y_test_sub_preds = []\n",
    "    for i in range(len(total_filled_preds)):\n",
    "        np_total_filled_pred = np.array(total_filled_preds[i])\n",
    "        y_test_sub_preds.append(pd.DataFrame(np_total_filled_pred, columns = dict_clst_col[i]))\n",
    "    y_test_pred = pd.concat(y_test_sub_preds, axis = 1)\n",
    "    y_test_pred_t = y_test_pred.T\n",
    "    y_test_pred_t_sorted = y_test_pred_t.sort_index(ascending=True)\n",
    "    y_test_pred_sorted = y_test_pred_t_sorted.T\n",
    "\n",
    "    return y_test_pred_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1a1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on splited X_train, y_train, calculate the y_test_pred on x_test\n",
    "\n",
    "def calc_preds(X_train, y_train, X_test, y_test, no_cls):\n",
    "    \n",
    "    X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    clf_super = super_classifier(X_train_, y_train_, no_cls)\n",
    "    clfs_sub, n_sub_labelses = sub_classifiers(X_train_, y_train_, no_cls)\n",
    "    y_test_s_pred = super_classification(clf_super, X_test_)\n",
    "    total_test_zeros, y_test_sub_preds = sub_classification(clfs_sub, n_sub_labelses, X_test_, y_test_s_pred)\n",
    "    total_filled_preds = fill_zeros(total_test_zeros, y_test_sub_preds, y_test_, n_sub_labelses)\n",
    "    y_test_pred_sorted = sort_preds(total_filled_preds, y_train_, no_cls)\n",
    "    \n",
    "    return y_test_pred_sorted, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611a548",
   "metadata": {},
   "source": [
    "#  Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682bc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold only accept X, y in numpy form, and return X_train, X_test, y_train, y_test in each loop(split).\n",
    "# Convert X_train, X_test, y_train, y_test from numpy to dataframe, for super_label calculation call.\n",
    "\n",
    "def Convert_to_df(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d091441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read arff file.\n",
    "\n",
    "def read_arff(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        header = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"@attribute\"):\n",
    "                header.append(line.split()[1])\n",
    "            elif line.startswith(\"@data\"):\n",
    "                break\n",
    "        df = pd.read_csv(f, header=None)\n",
    "        df.columns = header\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b51ff",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5358fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corel5k',\n",
       " 'bibtex',\n",
       " 'birds',\n",
       " 'delicious',\n",
       " 'emotions',\n",
       " 'enron',\n",
       " 'genbase',\n",
       " 'mediamill',\n",
       " 'medical',\n",
       " 'rcv1subset1',\n",
       " 'rcv1subset2',\n",
       " 'rcv1subset3',\n",
       " 'rcv1subset4',\n",
       " 'rcv1subset5',\n",
       " 'scene',\n",
       " 'tmc2007_500',\n",
       " 'yeast'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in available_data_sets().keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f135172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['CAL500', 'flags']\n",
    "n_featureses = [68, 19]\n",
    "n_labels = [174, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2615062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_small = ['emotions', 'scene']\n",
    "datasets_medium = ['yeast', 'birds', 'genbase']\n",
    "datasets_large = ['medical', 'enron', 'bibtex', 'Corel5k']\n",
    "datasets_list = [datasets_small, datasets_medium, datasets_large]\n",
    "# datasets_list = [datasets_medium]\n",
    "\n",
    "no_clses_small = [2, 4]\n",
    "no_clses_medium = [2, 4, 6, 8]\n",
    "no_clses_large = [2, 4, 6, 8, 10]\n",
    "no_clses_list = [no_clses_small, no_clses_medium, no_clses_large]\n",
    "# no_clses_list = [no_clses_medium]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cd563",
   "metadata": {},
   "source": [
    "Standard classification with full features vs Super classification with full features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee8e8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast:undivided - exists, not redownloading\n",
      "Split ============================================================================ 1\n",
      "no_cls is  2\n",
      "super_ham:  0.22294802612894063\n",
      "dict_cls_ham:  {2: 0.22294802612894063}\n",
      "no_cls is  4\n",
      "super_ham:  0.23132632774779893\n",
      "dict_cls_ham:  {2: 0.22294802612894063, 4: 0.23132632774779893}\n",
      "no_cls is  6\n",
      "super_ham:  0.22735018460664583\n",
      "dict_cls_ham:  {2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583}\n",
      "no_cls is  8\n",
      "super_ham:  0.2253621130360693\n",
      "dict_cls_ham:  {2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}\n",
      "super_hams:  [{2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.21953990343652371\n",
      "Split ============================================================================ 2\n",
      "no_cls is  2\n",
      "super_ham:  0.2206997084548105\n",
      "dict_cls_ham:  {2: 0.2206997084548105}\n",
      "no_cls is  4\n",
      "super_ham:  0.22740524781341107\n",
      "dict_cls_ham:  {2: 0.2206997084548105, 4: 0.22740524781341107}\n",
      "no_cls is  6\n",
      "super_ham:  0.22521865889212828\n",
      "dict_cls_ham:  {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828}\n",
      "no_cls is  8\n",
      "super_ham:  0.22317784256559767\n",
      "dict_cls_ham:  {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828, 8: 0.22317784256559767}\n",
      "super_hams:  [{2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}, {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828, 8: 0.22317784256559767}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.21793002915451895\n",
      "Split ============================================================================ 3\n",
      "no_cls is  2\n",
      "super_ham:  0.21739774748073504\n",
      "dict_cls_ham:  {2: 0.21739774748073504}\n",
      "no_cls is  4\n",
      "super_ham:  0.21932424422050978\n",
      "dict_cls_ham:  {2: 0.21739774748073504, 4: 0.21932424422050978}\n",
      "no_cls is  6\n",
      "super_ham:  0.21443390634262002\n",
      "dict_cls_ham:  {2: 0.21739774748073504, 4: 0.21932424422050978, 6: 0.21443390634262002}\n",
      "no_cls is  8\n",
      "super_ham:  0.21354475400118553\n",
      "dict_cls_ham:  {2: 0.21739774748073504, 4: 0.21932424422050978, 6: 0.21443390634262002, 8: 0.21354475400118553}\n",
      "super_hams:  [{2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}, {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828, 8: 0.22317784256559767}, {2: 0.21739774748073504, 4: 0.21932424422050978, 6: 0.21443390634262002, 8: 0.21354475400118553}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.20998814463544754\n",
      "Split ============================================================================ 4\n",
      "no_cls is  2\n",
      "super_ham:  0.22791938352104327\n",
      "dict_cls_ham:  {2: 0.22791938352104327}\n",
      "no_cls is  4\n",
      "super_ham:  0.23888559573206877\n",
      "dict_cls_ham:  {2: 0.22791938352104327, 4: 0.23888559573206877}\n",
      "no_cls is  6\n",
      "super_ham:  0.2354771784232365\n",
      "dict_cls_ham:  {2: 0.22791938352104327, 4: 0.23888559573206877, 6: 0.2354771784232365}\n",
      "no_cls is  8\n",
      "super_ham:  0.23147599288678128\n",
      "dict_cls_ham:  {2: 0.22791938352104327, 4: 0.23888559573206877, 6: 0.2354771784232365, 8: 0.23147599288678128}\n",
      "super_hams:  [{2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}, {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828, 8: 0.22317784256559767}, {2: 0.21739774748073504, 4: 0.21932424422050978, 6: 0.21443390634262002, 8: 0.21354475400118553}, {2: 0.22791938352104327, 4: 0.23888559573206877, 6: 0.2354771784232365, 8: 0.23147599288678128}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.22451096621221103\n",
      "Split ============================================================================ 5\n",
      "no_cls is  2\n",
      "super_ham:  0.21894409937888198\n",
      "dict_cls_ham:  {2: 0.21894409937888198}\n",
      "no_cls is  4\n",
      "super_ham:  0.2236024844720497\n",
      "dict_cls_ham:  {2: 0.21894409937888198, 4: 0.2236024844720497}\n",
      "no_cls is  6\n",
      "super_ham:  0.22142857142857142\n",
      "dict_cls_ham:  {2: 0.21894409937888198, 4: 0.2236024844720497, 6: 0.22142857142857142}\n",
      "no_cls is  8\n",
      "super_ham:  0.21940993788819876\n",
      "dict_cls_ham:  {2: 0.21894409937888198, 4: 0.2236024844720497, 6: 0.22142857142857142, 8: 0.21940993788819876}\n",
      "super_hams:  [{2: 0.22294802612894063, 4: 0.23132632774779893, 6: 0.22735018460664583, 8: 0.2253621130360693}, {2: 0.2206997084548105, 4: 0.22740524781341107, 6: 0.22521865889212828, 8: 0.22317784256559767}, {2: 0.21739774748073504, 4: 0.21932424422050978, 6: 0.21443390634262002, 8: 0.21354475400118553}, {2: 0.22791938352104327, 4: 0.23888559573206877, 6: 0.2354771784232365, 8: 0.23147599288678128}, {2: 0.21894409937888198, 4: 0.2236024844720497, 6: 0.22142857142857142, 8: 0.21940993788819876}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.21381987577639752\n",
      "birds:undivided - exists, not redownloading\n",
      "Split ============================================================================ 1\n",
      "no_cls is  2\n",
      "super_ham:  0.051407588739290085\n",
      "dict_cls_ham:  {2: 0.051407588739290085}\n",
      "no_cls is  4\n",
      "super_ham:  0.04732762137902897\n",
      "dict_cls_ham:  {2: 0.051407588739290085, 4: 0.04732762137902897}\n",
      "no_cls is  6\n",
      "super_ham:  0.04773561811505508\n",
      "dict_cls_ham:  {2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508}\n",
      "no_cls is  8\n",
      "super_ham:  0.04732762137902897\n",
      "dict_cls_ham:  {2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}\n",
      "super_hams:  [{2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.046511627906976744\n",
      "Split ============================================================================ 2\n",
      "no_cls is  2\n",
      "super_ham:  0.046103631170950635\n",
      "dict_cls_ham:  {2: 0.046103631170950635}\n",
      "no_cls is  4\n",
      "super_ham:  0.0485516115871073\n",
      "dict_cls_ham:  {2: 0.046103631170950635, 4: 0.0485516115871073}\n",
      "no_cls is  6\n",
      "super_ham:  0.04324765401876785\n",
      "dict_cls_ham:  {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785}\n",
      "no_cls is  8\n",
      "super_ham:  0.049367605059159526\n",
      "dict_cls_ham:  {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785, 8: 0.049367605059159526}\n",
      "super_hams:  [{2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}, {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785, 8: 0.049367605059159526}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.046511627906976744\n",
      "Split ============================================================================ 3\n",
      "no_cls is  2\n",
      "super_ham:  0.05222358221134231\n",
      "dict_cls_ham:  {2: 0.05222358221134231}\n",
      "no_cls is  4\n",
      "super_ham:  0.04814361485108119\n",
      "dict_cls_ham:  {2: 0.05222358221134231, 4: 0.04814361485108119}\n",
      "no_cls is  6\n",
      "super_ham:  0.0485516115871073\n",
      "dict_cls_ham:  {2: 0.05222358221134231, 4: 0.04814361485108119, 6: 0.0485516115871073}\n",
      "no_cls is  8\n",
      "super_ham:  0.053039575683394534\n",
      "dict_cls_ham:  {2: 0.05222358221134231, 4: 0.04814361485108119, 6: 0.0485516115871073, 8: 0.053039575683394534}\n",
      "super_hams:  [{2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}, {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785, 8: 0.049367605059159526}, {2: 0.05222358221134231, 4: 0.04814361485108119, 6: 0.0485516115871073, 8: 0.053039575683394534}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.051407588739290085\n",
      "Split ============================================================================ 4\n",
      "no_cls is  2\n",
      "super_ham:  0.04732762137902897\n",
      "dict_cls_ham:  {2: 0.04732762137902897}\n",
      "no_cls is  4\n",
      "super_ham:  0.04895960832313342\n",
      "dict_cls_ham:  {2: 0.04732762137902897, 4: 0.04895960832313342}\n",
      "no_cls is  6\n",
      "super_ham:  0.04814361485108119\n",
      "dict_cls_ham:  {2: 0.04732762137902897, 4: 0.04895960832313342, 6: 0.04814361485108119}\n",
      "no_cls is  8\n",
      "super_ham:  0.0485516115871073\n",
      "dict_cls_ham:  {2: 0.04732762137902897, 4: 0.04895960832313342, 6: 0.04814361485108119, 8: 0.0485516115871073}\n",
      "super_hams:  [{2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}, {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785, 8: 0.049367605059159526}, {2: 0.05222358221134231, 4: 0.04814361485108119, 6: 0.0485516115871073, 8: 0.053039575683394534}, {2: 0.04732762137902897, 4: 0.04895960832313342, 6: 0.04814361485108119, 8: 0.0485516115871073}]\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_ham:  0.04528763769889841\n",
      "Split ============================================================================ 5\n",
      "no_cls is  2\n",
      "super_ham:  0.049367605059159526\n",
      "dict_cls_ham:  {2: 0.049367605059159526}\n",
      "no_cls is  4\n",
      "super_ham:  0.049775601795185635\n",
      "dict_cls_ham:  {2: 0.049367605059159526, 4: 0.049775601795185635}\n",
      "no_cls is  6\n",
      "super_ham:  0.05059159526723786\n",
      "dict_cls_ham:  {2: 0.049367605059159526, 4: 0.049775601795185635, 6: 0.05059159526723786}\n",
      "no_cls is  8\n",
      "super_ham:  0.05344757241942064\n",
      "dict_cls_ham:  {2: 0.049367605059159526, 4: 0.049775601795185635, 6: 0.05059159526723786, 8: 0.05344757241942064}\n",
      "super_hams:  [{2: 0.051407588739290085, 4: 0.04732762137902897, 6: 0.04773561811505508, 8: 0.04732762137902897}, {2: 0.046103631170950635, 4: 0.0485516115871073, 6: 0.04324765401876785, 8: 0.049367605059159526}, {2: 0.05222358221134231, 4: 0.04814361485108119, 6: 0.0485516115871073, 8: 0.053039575683394534}, {2: 0.04732762137902897, 4: 0.04895960832313342, 6: 0.04814361485108119, 8: 0.0485516115871073}, {2: 0.049367605059159526, 4: 0.049775601795185635, 6: 0.05059159526723786, 8: 0.05344757241942064}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.049367605059159526\n",
      "genbase:undivided - exists, not redownloading\n",
      "Split ============================================================================ 1\n",
      "no_cls is  2\n",
      "super_ham:  0.0053717839977381965\n",
      "dict_cls_ham:  {2: 0.0053717839977381965}\n",
      "no_cls is  4\n",
      "super_ham:  0.0053717839977381965\n",
      "dict_cls_ham:  {2: 0.0053717839977381965, 4: 0.0053717839977381965}\n",
      "no_cls is  6\n",
      "super_ham:  0.005654509471303365\n",
      "dict_cls_ham:  {2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365}\n",
      "no_cls is  8\n",
      "super_ham:  0.005654509471303365\n",
      "dict_cls_ham:  {2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}\n",
      "super_hams:  [{2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.005089058524173028\n",
      "Split ============================================================================ 2\n",
      "no_cls is  2\n",
      "super_ham:  0.006357103372028745\n",
      "dict_cls_ham:  {2: 0.006357103372028745}\n",
      "no_cls is  4\n",
      "super_ham:  0.006357103372028745\n",
      "dict_cls_ham:  {2: 0.006357103372028745, 4: 0.006357103372028745}\n",
      "no_cls is  6\n",
      "super_ham:  0.006909894969596462\n",
      "dict_cls_ham:  {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462}\n",
      "no_cls is  8\n",
      "super_ham:  0.007186290768380321\n",
      "dict_cls_ham:  {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462, 8: 0.007186290768380321}\n",
      "super_hams:  [{2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}, {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462, 8: 0.007186290768380321}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.006633499170812604\n",
      "Split ============================================================================ 3\n",
      "no_cls is  2\n",
      "super_ham:  0.0028058361391694723\n",
      "dict_cls_ham:  {2: 0.0028058361391694723}\n",
      "no_cls is  4\n",
      "super_ham:  0.0030864197530864196\n",
      "dict_cls_ham:  {2: 0.0028058361391694723, 4: 0.0030864197530864196}\n",
      "no_cls is  6\n",
      "super_ham:  0.001964085297418631\n",
      "dict_cls_ham:  {2: 0.0028058361391694723, 4: 0.0030864197530864196, 6: 0.001964085297418631}\n",
      "no_cls is  8\n",
      "super_ham:  0.001964085297418631\n",
      "dict_cls_ham:  {2: 0.0028058361391694723, 4: 0.0030864197530864196, 6: 0.001964085297418631, 8: 0.001964085297418631}\n",
      "super_hams:  [{2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}, {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462, 8: 0.007186290768380321}, {2: 0.0028058361391694723, 4: 0.0030864197530864196, 6: 0.001964085297418631, 8: 0.001964085297418631}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.002244668911335578\n",
      "Split ============================================================================ 4\n",
      "no_cls is  2\n",
      "super_ham:  0.002544529262086514\n",
      "dict_cls_ham:  {2: 0.002544529262086514}\n",
      "no_cls is  4\n",
      "super_ham:  0.0033927056827820186\n",
      "dict_cls_ham:  {2: 0.002544529262086514, 4: 0.0033927056827820186}\n",
      "no_cls is  6\n",
      "super_ham:  0.0036754311563471868\n",
      "dict_cls_ham:  {2: 0.002544529262086514, 4: 0.0033927056827820186, 6: 0.0036754311563471868}\n",
      "no_cls is  8\n",
      "super_ham:  0.0031099802092168505\n",
      "dict_cls_ham:  {2: 0.002544529262086514, 4: 0.0033927056827820186, 6: 0.0036754311563471868, 8: 0.0031099802092168505}\n",
      "super_hams:  [{2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}, {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462, 8: 0.007186290768380321}, {2: 0.0028058361391694723, 4: 0.0030864197530864196, 6: 0.001964085297418631, 8: 0.001964085297418631}, {2: 0.002544529262086514, 4: 0.0033927056827820186, 6: 0.0036754311563471868, 8: 0.0031099802092168505}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.002544529262086514\n",
      "Split ============================================================================ 5\n",
      "no_cls is  2\n",
      "super_ham:  0.0030403537866224434\n",
      "dict_cls_ham:  {2: 0.0030403537866224434}\n",
      "no_cls is  4\n",
      "super_ham:  0.0024875621890547263\n",
      "dict_cls_ham:  {2: 0.0030403537866224434, 4: 0.0024875621890547263}\n",
      "no_cls is  6\n",
      "super_ham:  0.0024875621890547263\n",
      "dict_cls_ham:  {2: 0.0030403537866224434, 4: 0.0024875621890547263, 6: 0.0024875621890547263}\n",
      "no_cls is  8\n",
      "super_ham:  0.0024875621890547263\n",
      "dict_cls_ham:  {2: 0.0030403537866224434, 4: 0.0024875621890547263, 6: 0.0024875621890547263, 8: 0.0024875621890547263}\n",
      "super_hams:  [{2: 0.0053717839977381965, 4: 0.0053717839977381965, 6: 0.005654509471303365, 8: 0.005654509471303365}, {2: 0.006357103372028745, 4: 0.006357103372028745, 6: 0.006909894969596462, 8: 0.007186290768380321}, {2: 0.0028058361391694723, 4: 0.0030864197530864196, 6: 0.001964085297418631, 8: 0.001964085297418631}, {2: 0.002544529262086514, 4: 0.0033927056827820186, 6: 0.0036754311563471868, 8: 0.0031099802092168505}, {2: 0.0030403537866224434, 4: 0.0024875621890547263, 6: 0.0024875621890547263, 8: 0.0024875621890547263}]\n",
      "-------------------------------------------------------------\n",
      "standard_ham:  0.002763957987838585\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "# for dataset, n_features in zip(datasets, n_featureses):\n",
    "    \n",
    "#     data = read_arff('datasets/' + dataset + '.arff')\n",
    "\n",
    "#     y = data.iloc[:10000, n_features:]\n",
    "#     X = data.iloc[:10000, :n_features]\n",
    "\n",
    "# #     n_features = len(list(X))\n",
    "#     X = X.to_numpy()\n",
    "#     y = y.to_numpy()\n",
    "\n",
    "for datasets, no_clses in zip(datasets_list, no_clses_list):\n",
    "    for i in range(len(datasets)):\n",
    "        X, y, feature_names, label_names = load_dataset(datasets[i], 'undivided')\n",
    "        X = pd.DataFrame.sparse.from_spmatrix(X).to_numpy()\n",
    "        y = pd.DataFrame.sparse.from_spmatrix(y).to_numpy()\n",
    "\n",
    "        n_splits = 5\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "        standard_hams = []\n",
    "        super_hams = []\n",
    "        to_print = ''\n",
    "        fold_count = 0\n",
    "\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            fold_count += 1\n",
    "            print('Split ============================================================================', fold_count)\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "            # get super classification acc(hl)\n",
    "            dict_cls_ham = dict()   # no_cls:super_ham dictionary\n",
    "            for no_cls in no_clses:   \n",
    "                print('no_cls is ', no_cls)\n",
    "                X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "                y_test_pred_super, y_test_super = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df, no_cls)\n",
    "                super_ham = hamming_loss(y_test_super, y_test_pred_super)        \n",
    "                dict_cls_ham[no_cls] = super_ham\n",
    "                print('super_ham: ', super_ham)\n",
    "                print('dict_cls_ham: ', dict_cls_ham)\n",
    "            super_hams.append(dict_cls_ham)\n",
    "            print('super_hams: ', super_hams)\n",
    "            print(\"-------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "            # get standard classficaition acc(hl)\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "            clf = MLkNN(k=3)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            y_test_pred = clf.predict(X_test_scaled)\n",
    "            standard_ham = hamming_loss(y_test, y_test_pred)\n",
    "            print('standard_ham: ', standard_ham)\n",
    "            standard_hams.append(standard_ham)\n",
    "\n",
    "            # to write the results\n",
    "            to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "            to_print += 'Standard classification with full feature hamming loss: %.4f\\n' % standard_ham\n",
    "            if len(no_clses) == 2:\n",
    "                to_print += 'Super classification with full feature hamming loss with 2 clusters: %.4f\\n' % dict_cls_ham[2]\n",
    "                to_print += 'Super classification with full feature hamming loss with 4 clusters: %.4f\\n' % dict_cls_ham[4]\n",
    "            elif len(no_clses) == 4:\n",
    "                to_print += 'Super classification with full feature hamming loss with 2 clusters: %.4f\\n' % dict_cls_ham[2]\n",
    "                to_print += 'Super classification with full feature hamming loss with 4 clusters: %.4f\\n' % dict_cls_ham[4]\n",
    "                to_print += 'Super classification with full feature hamming loss with 6 clusters: %.4f\\n' % dict_cls_ham[6]\n",
    "                to_print += 'Super classification with full feature hamming loss with 8 clusters: %.4f\\n' % dict_cls_ham[8]\n",
    "            else:\n",
    "                to_print += 'Super classification with full feature hamming loss with 2 clusters: %.4f\\n' % dict_cls_ham[2]\n",
    "                to_print += 'Super classification with full feature hamming loss with 4 clusters: %.4f\\n' % dict_cls_ham[4]\n",
    "                to_print += 'Super classification with full feature hamming loss with 6 clusters: %.4f\\n' % dict_cls_ham[6]\n",
    "                to_print += 'Super classification with full feature hamming loss with 8 clusters: %.4f\\n' % dict_cls_ham[8]\n",
    "                to_print += 'Super classification with full feature hamming loss with 10 clusters: %.4f\\n' % dict_cls_ham[10]\n",
    "\n",
    "        to_print += '--------------Average----------------\\n'\n",
    "        to_print += 'Ave Standard Classification Accuracy: %.4f\\n' % np.average(standard_hams)\n",
    "        # to_print += 'Ave Super Classification Accuracy: %.4f\\n' % np.average(super_hams)\n",
    "        if len(no_clses) == 2:\n",
    "            to_print += 'Ave Super Classification with 2 clusters: %.4f\\n' % np.average([super_hams[0][2], super_hams[1][2], super_hams[2][2], super_hams[3][2], super_hams[4][2]])\n",
    "            to_print += 'Ave Super Classification with 4 clusters: %.4f\\n' % np.average([super_hams[0][4], super_hams[1][4], super_hams[2][4], super_hams[3][4], super_hams[4][4]])\n",
    "        elif len(no_clses) == 4:\n",
    "            to_print += 'Ave Super Classification with 2 clusters: %.4f\\n' % np.average([super_hams[0][2], super_hams[1][2], super_hams[2][2], super_hams[3][2], super_hams[4][2]])\n",
    "            to_print += 'Ave Super Classification with 4 clusters: %.4f\\n' % np.average([super_hams[0][4], super_hams[1][4], super_hams[2][4], super_hams[3][4], super_hams[4][4]])\n",
    "            to_print += 'Ave Super Classification with 6 clusters: %.4f\\n' % np.average([super_hams[0][6], super_hams[1][6], super_hams[2][6], super_hams[3][6], super_hams[4][6]])\n",
    "            to_print += 'Ave Super Classification with 8 clusters: %.4f\\n' % np.average([super_hams[0][8], super_hams[1][8], super_hams[2][8], super_hams[3][8], super_hams[4][8]])\n",
    "        else:\n",
    "            to_print += 'Ave Super Classification with 2 clusters: %.4f\\n' % np.average([super_hams[0][2], super_hams[1][2], super_hams[2][2], super_hams[3][2], super_hams[4][2]])\n",
    "            to_print += 'Ave Super Classification with 4 clusters: %.4f\\n' % np.average([super_hams[0][4], super_hams[1][4], super_hams[2][4], super_hams[3][4], super_hams[4][4]])\n",
    "            to_print += 'Ave Super Classification with 6 clusters: %.4f\\n' % np.average([super_hams[0][6], super_hams[1][6], super_hams[2][6], super_hams[3][6], super_hams[4][6]])\n",
    "            to_print += 'Ave Super Classification with 8 clusters: %.4f\\n' % np.average([super_hams[0][8], super_hams[1][8], super_hams[2][8], super_hams[3][8], super_hams[4][8]])\n",
    "            to_print += 'Ave Super Classification with 10 clusters: %.4f\\n' % np.average([super_hams[0][10], super_hams[1][10], super_hams[2][10], super_hams[3][10], super_hams[4][10]])\n",
    "\n",
    "        f = open('records/record_stdscaler_' + datasets[i] + '_full_standard_super_clf.txt', 'w')\n",
    "        f.write(to_print)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "baf1e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20e0c2",
   "metadata": {},
   "source": [
    "Ave Standard Classification Accuracy: 0.2153\n",
    "Ave Super Classification with 2 clusters: 0.2181\n",
    "Ave Super Classification with 4 clusters: 0.2111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4895a086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Accuracy (Hamming_loss)') \n",
    "plt.axhline(y=0.2153, color='r', linestyle='-')\n",
    "x_values = [2, 4]\n",
    "y_values = [0.2181, 0.2111]\n",
    "plt.plot(x_values, y_values) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52580cf4",
   "metadata": {},
   "source": [
    "rd Classification Accuracy: 0.0923\n",
    "Ave Super Classification with 2 clusters: 0.0923\n",
    "Ave Super Classification with 4 clusters: 0.0951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6718edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Accuracy (Hamming_loss)') \n",
    "plt.axhline(y=0.0923, color='r', linestyle='-')\n",
    "x_values = [2, 4]\n",
    "y_values = [0.0923, 0.0951]\n",
    "plt.plot(x_values, y_values) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172e33d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a59552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7702a97f",
   "metadata": {},
   "source": [
    "Full feature standard classfification acc vs Super classification PSO selected acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42eda467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10470910472386\n",
      "Iterate  2  gbest value is  0.10166311485847494\n",
      "Iterate  3  gbest value is  0.10166311485847494\n",
      "Iterate  4  gbest value is  0.10166311485847494\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.1047325435269018\n",
      "Iterate  2  gbest value is  0.10452233554469884\n",
      "Iterate  3  gbest value is  0.10154203061304254\n",
      "Iterate  4  gbest value is  0.10154203061304254\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09831199379037225\n",
      "Iterate  2  gbest value is  0.09831199379037225\n",
      "Iterate  3  gbest value is  0.09804015334541763\n",
      "Iterate  4  gbest value is  0.09804015334541763\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09906150874510802\n",
      "Iterate  2  gbest value is  0.09849433193971643\n",
      "Iterate  3  gbest value is  0.09849433193971643\n",
      "Iterate  4  gbest value is  0.09849433193971643\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10409872338665445\n",
      "Iterate  2  gbest value is  0.09669688336355002\n",
      "Iterate  3  gbest value is  0.09669688336355002\n",
      "Iterate  4  gbest value is  0.09669688336355002\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "# y = data.iloc[:, locations[6]:]\n",
    "# X = data.iloc[:, :locations[6]]\n",
    "# n_features = len(list(X))\n",
    "# X = X.to_numpy()\n",
    "# y = y.to_numpy()\n",
    "\n",
    "X, y, feature_names, label_names = load_dataset(datasets_large[0], 'undivided')\n",
    "X = pd.DataFrame.sparse.from_spmatrix(X).to_numpy()\n",
    "y = pd.DataFrame.sparse.from_spmatrix(y).to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "#        problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "    problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_super_PSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38420eaa",
   "metadata": {},
   "source": [
    "Standard PSO FS time cost vs Super PSO FS time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10053697819836607\n",
      "Iterate  2  gbest value is  0.10053697819836607\n",
      "Iterate  3  gbest value is  0.10053697819836607\n",
      "Iterate  4  gbest value is  0.10053697819836607\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10302277941047297\n",
      "Iterate  2  gbest value is  0.10302277941047297\n",
      "Iterate  3  gbest value is  0.10302277941047297\n",
      "Iterate  4  gbest value is  0.10138353021981936\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.0962167647246712\n",
      "Iterate  2  gbest value is  0.0962167647246712\n",
      "Iterate  3  gbest value is  0.0962167647246712\n",
      "Iterate  4  gbest value is  0.0962167647246712\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10309103367896781\n",
      "Iterate  2  gbest value is  0.10309103367896781\n",
      "Iterate  3  gbest value is  0.10309103367896781\n",
      "Iterate  4  gbest value is  0.09910224523650166\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10233582798051337\n",
      "Iterate  2  gbest value is  0.10233582798051337\n",
      "Iterate  3  gbest value is  0.10233582798051337\n",
      "Iterate  4  gbest value is  0.10233582798051337\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "    problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "#     problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_standardPSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
