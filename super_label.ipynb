{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686f4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from skmultilearn.dataset import available_data_sets\n",
    "from skmultilearn.dataset import load_dataset\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5388",
   "metadata": {},
   "source": [
    "#  Particle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8247ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    \n",
    "    def __init__(self, length, pos_max, pos_min, vel_max, vel_min, w, c1, c2, problem):\n",
    "        self.length = length\n",
    "        self.pos_max = pos_max\n",
    "        self.pos_min = pos_min\n",
    "        self.vel_max = vel_max\n",
    "        self.vel_min = vel_min\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.problem = problem\n",
    "\n",
    "        self.position = np.random.rand(length)*(pos_max-pos_min)\n",
    "        self.velocity = np.zeros(length)\n",
    "        self.fitness = self.problem.worst_fitness()\n",
    "\n",
    "        self.pbest_pos = np.zeros(length)\n",
    "        self.pbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "        self.gbest_pos = np.zeros(length)\n",
    "        self.gbest_fit = self.problem.worst_fitness()\n",
    "\n",
    "    def update(self):\n",
    "        # Update velocity\n",
    "        self.velocity = self.w * self.velocity + \\\n",
    "            self.c1 * np.random.rand(self.length) * (self.pbest_pos - self.position) + \\\n",
    "            self.c2 * np.random.rand(self.length) * \\\n",
    "            (self.gbest_pos - self.position)\n",
    "\n",
    "        self.velocity[self.velocity < self.vel_min] = self.vel_min\n",
    "        self.velocity[self.velocity > self.vel_max] = self.vel_max\n",
    "\n",
    "        # update position\n",
    "        self.position = self.position + self.velocity\n",
    "        self.position[self.position < self.pos_min] = self.pos_min\n",
    "        self.position[self.position > self.pos_max] = self.pos_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "\n",
    "    def __init__(self, n_particles, length, pos_max, pos_min, vel_max, vel_min, problem, n_iterations):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.problem = problem\n",
    "\n",
    "        w = 0.8\n",
    "        c1 = 1.46\n",
    "        c2 = 1.46\n",
    "        self.population = [Particle(length = length, \n",
    "                                    pos_max = pos_max, pos_min = pos_min, \n",
    "                                    vel_max = vel_max, vel_min = vel_min, \n",
    "                                    w = w, c1 = c1, c2 = c2, problem = problem)\n",
    "                           for _ in range(n_particles)]\n",
    "\n",
    "    def iterate(self):\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            print('Iterate ', i, end = '  ')\n",
    "            gbest_fit = self.population[0].gbest_fit\n",
    "            gbest_index = 0\n",
    "            gbest_updated = False\n",
    "            print('gbest value is ', gbest_fit)\n",
    "            \n",
    "            for index, particle in enumerate(self.population):\n",
    "                # Evaluate each particle, update pbest\n",
    "                particle.fitness = self.problem.fitness(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.fitness, particle.pbest_fit):\n",
    "                    particle.pbest_fit = particle.fitness\n",
    "                    particle.pbest_pos = np.copy(particle.position)\n",
    "\n",
    "                if self.problem.is_better(particle.pbest_fit, gbest_fit):\n",
    "                    gbest_fit = particle.pbest_fit\n",
    "                    gbest_index = index\n",
    "                    gbest_updated = True\n",
    "\n",
    "            if gbest_updated:\n",
    "                for particle in self.population:\n",
    "                    particle.gbest_fit = self.population[gbest_index].pbest_fit\n",
    "                    particle.gbest_pos = np.copy(\n",
    "                        self.population[gbest_index].pbest_pos)\n",
    "\n",
    "            # now update particle position:\n",
    "            for particle in self.population:\n",
    "                particle.update()\n",
    "\n",
    "        return self.population[0].gbest_pos, self.population[0].gbest_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330052b8",
   "metadata": {},
   "source": [
    "#  Problem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bcdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def __init__(self, minimize):\n",
    "        self.minimize = minimize\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        return 1\n",
    "    \n",
    "    def is_better(self, first, second):\n",
    "        if self.minimize:\n",
    "            return first < second\n",
    "        else:\n",
    "            return first > second\n",
    "\n",
    "    def worst_fitness(self):\n",
    "        if self.minimize:\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FS(Problem):\n",
    "\n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        clf = KNN()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_pred, y_test)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a1bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PSO\n",
    "# Fitness is MLKNN classification hamming loss.\n",
    "\n",
    "class FS_ML(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        clf = MLkNN(k=3)\n",
    "        scaler = StandardScaler()\n",
    "#         scaler = MinMaxScaler()\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            ham = hamming_loss(y_test, y_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super label PSO\n",
    "# Fitness is use super+sub classification hamming loss\n",
    "\n",
    "class FS_ML_super(Problem):\n",
    "    \n",
    "    def __init__(self, minimize, X, y):\n",
    "        self.minimize = minimize,\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.threshold = 0.6\n",
    "\n",
    "    def fitness(self, solution):\n",
    "        feature_selected = np.where(solution > self.threshold)[0]\n",
    "        X = self.X[:, feature_selected]\n",
    "        y = self.y\n",
    "        if len(feature_selected) == 0:\n",
    "            return self.worst_fitness()\n",
    "        \n",
    "        n_splits = 5\n",
    "        k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "            \n",
    "        hamming_losses = 0\n",
    "        for train_idx, test_idx in k_fold.split(X, y):\n",
    "            \n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "            y_test_pred, y_test = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "            \n",
    "        \n",
    "            ham = hamming_loss(y_test, y_test_pred)\n",
    "            hamming_losses += ham\n",
    "        \n",
    "        return hamming_losses/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776309eb",
   "metadata": {},
   "source": [
    "# Super_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5172b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subcgroup(cluster), get label indexes \n",
    "\n",
    "def ClusterIndicesNumpy(clustNum, labels_array): #numpy \n",
    "    return np.where(labels_array == clustNum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ee72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subgroup's labels(binary nparray) into super label(list)\n",
    "# If all labels are 0, super label is 0; otherwise, super label is assigned to 1\n",
    "\n",
    "def convert(subgroup_label):\n",
    "    super_ = []\n",
    "    subgroup_label_array = subgroup_label.to_numpy()\n",
    "    rows = subgroup_label.shape[0]\n",
    "    columns = subgroup_label.shape[1]\n",
    "    for row in range(rows):\n",
    "        s = 0\n",
    "        for column in range(columns):\n",
    "            if subgroup_label_array[row][column] == 1:\n",
    "                s = 1\n",
    "                break\n",
    "        super_.append(s)\n",
    "    return super_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25299393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original y labels into y_super labels\n",
    "# subgroup_labels are dataframe with original labels + super labels\n",
    "\n",
    "def label_convert(y_train_, no_cls):  # Here y is dataframe\n",
    "    \n",
    "    subgroups = []\n",
    "    super_labels = []\n",
    "    kmeans = KMeans(n_clusters=no_cls, random_state=0).fit(y_train_.T)\n",
    "    dict_clst_col = dict()  # dictionary to record key(cluster index) and value(cluster columns)\n",
    "    \n",
    "    for i in range(no_cls):\n",
    "        cluster = ClusterIndicesNumpy(i, kmeans.labels_)  # Column numbers(indexes) of all the labels in each cluster\n",
    "        dict_clst_col[i] = cluster\n",
    "        subgroup_label = y_train_.iloc[:,cluster]   # Get all the original labels from cluster, dataframe form\n",
    "        s = pd.DataFrame(convert(subgroup_label), columns = ['s'+ str(i)])   # Convert original labels to a column super label\n",
    "        super_labels.append(s)\n",
    "        subgroup_label['s' + str(i)] = s   # Concat s into subgroup\n",
    "        subgroups.append(subgroup_label)        \n",
    "        \n",
    "    y_s = pd.concat(super_labels, axis=1)   # Combine all super label columns, as orginal y converted to super_label y, the target\n",
    "    y_s = y_s.to_numpy()\n",
    "\n",
    "    return y_s, subgroups, dict_clst_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eeec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split, indexes of X_train, X_test and y_train, y_test will be discorder, aka not ascending any more.\n",
    "# If index disorder, will be tricky to process index, e.g. zero_idx\n",
    "# Need to reorder index first.\n",
    "\n",
    "def convert_index(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_ = X_train.reset_index(drop=True)\n",
    "    y_train_ = y_train.reset_index(drop=True)\n",
    "    X_test_ = X_test.reset_index(drop=True)\n",
    "    y_test_ = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train_, X_test_, y_train_, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103116b4",
   "metadata": {},
   "source": [
    "When doing super and sub classification on training set, k-fold is not neccesary. \n",
    "Only the classifiers are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c970bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained super_classifier\n",
    "\n",
    "def super_classifier(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ = scaler.fit_transform(X_train_)\n",
    "#     X_train_ = MinMaxScaler().fit_transform(X_train_)\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    clf.fit(X_train_, y_s)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subgroup, collect all the zero super labels\n",
    "# The len of total_zeros is the number of subgroups, also the number of super labels\n",
    "\n",
    "def zeros(y_s):\n",
    "\n",
    "    total_zeros = []\n",
    "    for i in range(y_s.shape[1]):     # number of super labels\n",
    "        idx_zeros = []\n",
    "        for j in range(y_s.shape[0]):   # number of instances\n",
    "            if y_s[j][i] == 0:\n",
    "                idx_zeros.append(j)\n",
    "        total_zeros.append(idx_zeros)\n",
    "        \n",
    "    return total_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ab36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each subgroup, if a particular row of y_s is zero, the corresponding X features also needs to be removed.\n",
    "# For each subgroup, get the indexes of zeros in one y_s, and remove these same indexes from X feature instances.\n",
    "# Each subgroup contains its own X, means different subgroup contain diffenrent number of instances\n",
    "# Collect each removed X and return.\n",
    "\n",
    "def remove_zeros(X, y_s):  # y_s is ndarray\n",
    "    total_zeros = zeros(y_s)\n",
    "    Xs = []\n",
    "    for idx_zeros in total_zeros:\n",
    "        X_ = pd.DataFrame(X).drop(idx_zeros)   \n",
    "        Xs.append(X_)  \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9681",
   "metadata": {},
   "source": [
    "From original X and y, compute super label y(y_s), which actually comes from true y.\n",
    "Each subgroup contains original y labels + y_s label.\n",
    "Check each y_s, if 0, than remove the whole line, which means remove its corresponding original labels, and its X.\n",
    "So the remaining of original labels, as well as X of each subgroup are different, since indexes of zeros in each y super label are different.\n",
    "\n",
    "def sub_classification is for each subgroup, train X_(X remove y_s's zero indexes) and y_(y sub original labels remove y_s's zero indexes).\n",
    "After training, collect all sub-clfs and Xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained sub classifiers\n",
    "\n",
    "def sub_classifiers(X_train_, y_train_, no_cls):\n",
    "    \n",
    "    clfs = []\n",
    "    y_s, subgroups, dict_clst_col = label_convert(y_train_, no_cls)   # y_super labels, converted from original y labels (target)\n",
    "    total_zeros = zeros(y_s)\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    Xs = remove_zeros(X_train_, y_s)\n",
    "    \n",
    "    for subgroup, idx_zeros, X in zip(subgroups,total_zeros, Xs):\n",
    "                                                 # Have different X, because idx of zero are different\n",
    "        y_ = subgroup.drop(idx_zeros)            # Drop all the zero instances, both in X and y, aka X_, y_ \n",
    "        y_ = y_.drop(y_.columns[-1:], axis = 1)  # Remove the s label\n",
    "        \n",
    "        X_ = scaler.fit_transform(X)\n",
    "        clf = MLkNN(k=3)\n",
    "#         clf.fit(X, y_.to_numpy())\n",
    "        clf.fit(X_, y_.to_numpy())\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    return clfs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d6bd",
   "metadata": {},
   "source": [
    "Now already gained clf, which is classifier for super classification, and clfs which are for all the sub-classifications.\n",
    "Then will apply clf and clfs on training set, to see the training_loss, and then apply on test set, to get test_loss.\n",
    "Finally, compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63521ca",
   "metadata": {},
   "source": [
    "1. Apply clf, clfs, Xs on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d2614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do super classification on dataset\n",
    "\n",
    "def super_classification(clf_super, X_test_):\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test_)\n",
    "#     X_test_scaled = MinMaxScaler().fit_transform(X_test_)\n",
    "    y_test_s_pred = clf_super.predict(X_test_scaled).toarray()   # Predicted super labels, will be passed into def zeros().\n",
    "#     y_test_s_pred = clf_super.predict(X_test_).toarray()\n",
    "    return y_test_s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "759395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do sub-classification on sub-datasets (original X + subgroup original labels)\n",
    "\n",
    "def sub_classification(clfs, X_test_, y_test_s_pred):\n",
    "\n",
    "    total_test_zeros = zeros(y_test_s_pred)    # Based on predicted super label, compute which are zeros in each subgroup\n",
    "    \n",
    "    X_tests = remove_zeros(X_test_, y_test_s_pred)  # Remove zeros in each subgroup in X\n",
    "    \n",
    "    y_test_sub_preds = []\n",
    "    scaler = StandardScaler() \n",
    "#     scaler = MinMaxScaler()\n",
    "    for clf, X_test in zip(clfs, X_tests):\n",
    "        X_scaled = scaler.fit_transform(X_test)\n",
    "        y_test_sub_pred = clf.predict(X_scaled)\n",
    "#         y_test_sub_pred = clf.predict(X_test_)\n",
    "        y_test_sub_preds.append(y_test_sub_pred)\n",
    "        \n",
    "    return total_test_zeros, y_test_sub_preds    # total_test_zeros, y_test_sub_labels are lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb8a5b",
   "metadata": {},
   "source": [
    "After super and sub-classifications are done on test/validation dataset, the next step is to revert and rebuild all the predicted sub-labels together. \n",
    "The predicted subgroups do not contain all the original instances, coz those all-zeros instances are removed before sub-classification. So when doing revert, we need to find out which instances are all-zeros(those predicted super-predicted are zero), these can be reverted to [0,0,0,...].\n",
    "If the super-predicted label is not 0, then this predicted instance's subgroup labels are in coreesponding y_sub_pred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdea6",
   "metadata": {},
   "source": [
    "len(total_zeros) is number of subgroups, aka number of columns for super_y_labels\n",
    "for each subgroup, y.shape[0] is the rows, aka instances in original y, \n",
    "if index of the instance is included in column in total_zeros, that means when revert to original labels, we can impute all the subgroup labels of this instance to all zeros\n",
    "\n",
    "How to impute the zero super label's corresponding sub-labels to zeros? We need to know how many sub-labels in each cluster. That is how many labels in each y_sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "382616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out zeros back into each y_sub_pred\n",
    "\n",
    "def fill_zeros(total_test_zeros, y_test_sub_preds, y_test_):\n",
    "    total_filled_preds = []\n",
    "    \n",
    "    for i in range(len(total_test_zeros)):   # number of columns of y_super\n",
    "        labels = []        \n",
    "        y_test_sub_pred_np = y_test_sub_preds[i].toarray()\n",
    "        no_sublabels = y_test_sub_pred_np.shape[1]\n",
    "        for j in range(y_test_.shape[0]):   # original y_test's rows\n",
    "            if j in total_test_zeros[i]:   \n",
    "                labels.append(np.zeros(no_sublabels, dtype=np.int64))                                          \n",
    "            else:\n",
    "                label = y_test_sub_pred_np[0]             \n",
    "                labels.append(label)\n",
    "                y_test_sub_pred_np = np.delete(y_test_sub_pred_np, [0], axis = 0)\n",
    "        total_filled_preds.append(labels)  \n",
    "        \n",
    "    return total_filled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the order of y_test_preds, aka revert to original order as y_test's columns(before clustering)\n",
    "# Clusters are not neccessarily equal, so could convert total_filled_preds to np.array\n",
    "# If cluster0 is 2, cluster is 4, will cause passed value index issuse.\n",
    "# Reference to backup_0113 version.\n",
    "\n",
    "def sort_preds(total_filled_preds, y_train_, no_cls):\n",
    "    \n",
    "    y_s, subgroup, dict_clst_col = label_convert(y_train_, no_cls)\n",
    "    y_test_sub_preds = []\n",
    "    for i in range(len(total_filled_preds)):\n",
    "        np_total_filled_pred = np.array(total_filled_preds[i])\n",
    "        y_test_sub_preds.append(pd.DataFrame(np_total_filled_pred, columns = dict_clst_col[i]))\n",
    "    y_test_pred = pd.concat(y_test_sub_preds, axis = 1)\n",
    "    y_test_pred_t = y_test_pred.T\n",
    "    y_test_pred_t_sorted = y_test_pred_t.sort_index(ascending=True)\n",
    "    y_test_pred_sorted = y_test_pred_t_sorted.T\n",
    "\n",
    "    return y_test_pred_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1a1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on splited X_train, y_train, calculate the y_test_pred on x_test\n",
    "\n",
    "def calc_preds(X_train, y_train, X_test, y_test, no_cls):\n",
    "    \n",
    "    X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    clf_super = super_classifier(X_train_, y_train_, no_cls)\n",
    "    clfs_sub = sub_classifiers(X_train_, y_train_, no_cls)\n",
    "    y_test_s_pred = super_classification(clf_super, X_test_)\n",
    "    total_test_zeros, y_test_sub_preds = sub_classification(clfs_sub, X_test_, y_test_s_pred)\n",
    "    total_filled_preds = fill_zeros(total_test_zeros, y_test_sub_preds, y_test_)\n",
    "    y_test_pred_sorted = sort_preds(total_filled_preds, y_train_, no_cls)\n",
    "    \n",
    "    return y_test_pred_sorted, y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611a548",
   "metadata": {},
   "source": [
    "#  Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682bc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold only accept X, y in numpy form, and return X_train, X_test, y_train, y_test in each loop(split).\n",
    "# Convert X_train, X_test, y_train, y_test from numpy to dataframe, for super_label calculation call.\n",
    "\n",
    "def Convert_to_df(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d539a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experimental function.\n",
    "# # To find out if cluster is not equal, what will happen.\n",
    "\n",
    "# def get_not_three(X, y):\n",
    "#     for i in range(5000):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "#         X_train_, X_test_, y_train_, y_test_ = convert_index(X_train, X_test, y_train, y_test)\n",
    "#         kmeans = KMeans(n_clusters=2).fit(y_train_.T)\n",
    "#         cluster = ClusterIndicesNumpy(0, kmeans.labels_)\n",
    "#         if len(cluster) == 3:\n",
    "#             print(i, cluster)\n",
    "#             return X_train_, X_test_, y_train_, y_test_, kmeans\n",
    "#     print('No cluster equals to one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d091441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read arff file.\n",
    "\n",
    "# def read_arff(file):\n",
    "#     with open(file, encoding=\"utf-8\") as f:\n",
    "#         header = []\n",
    "#         for line in f:\n",
    "#             if line.startswith(\"@attribute\"):\n",
    "#                 header.append(line.split()[1])\n",
    "#             elif line.startswith(\"@data\"):\n",
    "#                 break\n",
    "#         df = pd.read_csv(f, header=None)\n",
    "#         df.columns = header\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88593597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52d55230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import arff\n",
    "# import pandas as pd\n",
    "\n",
    "# data = arff.loadarff('datasets/emotions.arff')\n",
    "# df = pd.DataFrame(data[0])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b51ff",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaa3a4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corel5k',\n",
       " 'bibtex',\n",
       " 'birds',\n",
       " 'delicious',\n",
       " 'emotions',\n",
       " 'enron',\n",
       " 'genbase',\n",
       " 'mediamill',\n",
       " 'medical',\n",
       " 'rcv1subset1',\n",
       " 'rcv1subset2',\n",
       " 'rcv1subset3',\n",
       " 'rcv1subset4',\n",
       " 'rcv1subset5',\n",
       " 'scene',\n",
       " 'tmc2007_500',\n",
       " 'yeast'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in available_data_sets().keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f135172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['birds-train', 'CAL500', 'emotions', 'enron', 'flags', 'medical', 'scene', 'yeast', 'Corel5k', 'mediamill']\n",
    "locations = [260, 68, 72, 1001, 19, 1449, 294, 103, 499, 120]\n",
    "n_labels = [19, 174, 6, 53, 7, 45, 6, 14, 374, 101]\n",
    "no_clses = [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c0811a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_small = ['emotions', 'scene']\n",
    "datasets_medium = ['yeast', 'birds', 'genbase']\n",
    "datasets_large = ['medical', 'enron']\n",
    "datasets_huge = ['mediamill', 'bibtex', 'Corel5k']\n",
    "no_clses_small = [2]\n",
    "no_clses_medium = [2, 4, 6]\n",
    "no_clses_large = [2, 4, 6, 8, 10]\n",
    "no_clses_huge = [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21427bb",
   "metadata": {},
   "source": [
    "Standard classification with full features vs Super classification with full features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b49d427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast:undivided - does not exists downloading\n",
      "Downloaded yeast-undivided\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "# data = read_arff('datasets/' + datasets[2] + '.arff')\n",
    "\n",
    "# y = data.iloc[:, locations[2]:]\n",
    "# X = data.iloc[:, :locations[2]]\n",
    "\n",
    "# n_features = len(list(X))\n",
    "# X = X.to_numpy()\n",
    "# y = y.to_numpy()\n",
    "\n",
    "X, y, feature_names, label_names = load_dataset(datasets_medium[0], 'undivided')\n",
    "X = pd.DataFrame.sparse.from_spmatrix(X).to_numpy()\n",
    "y = pd.DataFrame.sparse.from_spmatrix(y).to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state=42)\n",
    "\n",
    "standard_hams = []\n",
    "super_hams = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get super classification acc(hl)\n",
    "    dict_cls_ham = dict()   # no_cls:super_ham dictionary\n",
    "    for no_cls in no_clses_medium:        \n",
    "        X_train_df, X_test_df, y_train_df, y_test_df = Convert_to_df(X_train, X_test, y_train, y_test)            \n",
    "        y_test_pred_super, y_test_super = calc_preds(X_train_df, y_train_df, X_test_df, y_test_df, no_cls)\n",
    "        super_ham = hamming_loss(y_test_super, y_test_pred_super)        \n",
    "        dict_cls_ham[no_cls] = super_ham\n",
    "        super_hams.append(dict_cls_ham)\n",
    "    \n",
    "    # get standard classficaition acc(hl)\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    standard_ham = hamming_loss(y_test, y_test_pred)\n",
    "    standard_hams.append(standard_ham)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Standard classification with full feature hamming loss: %.4f\\n' % standard_ham\n",
    "    to_print += 'Super classification with full feature hamming loss with 2 clusters: %.4f\\n' % dict_cls_ham[2]\n",
    "    to_print += 'Super classification with full feature hamming loss with 4 clusters: %.4f\\n' % dict_cls_ham[4]\n",
    "    to_print += 'Super classification with full feature hamming loss with 6 clusters: %.4f\\n' % dict_cls_ham[6]\n",
    "#     to_print += 'Super classification with full feature hamming loss with 8 clusters: %.4f\\n' % dict_cls_ham[8]\n",
    "#     to_print += 'Super classification with full feature hamming loss with 10 clusters: %.4f\\n' % dict_cls_ham[10]\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Standard Classification Accuracy: %.4f\\n' % np.average(standard_hams)\n",
    "# to_print += 'Ave Super Classification Accuracy: %.4f\\n' % np.average(super_hams)\n",
    "to_print += 'Ave Super Classification with 2 clusters: %.4f\\n' % np.average([super_hams[0][2], super_hams[1][2], super_hams[2][2], super_hams[3][2], super_hams[4][2]])\n",
    "to_print += 'Ave Super Classification with 4 clusters: %.4f\\n' % np.average([super_hams[0][4], super_hams[1][4], super_hams[2][4], super_hams[3][4], super_hams[4][4]])\n",
    "to_print += 'Ave Super Classification with 6 clusters: %.4f\\n' % np.average([super_hams[0][6], super_hams[1][6], super_hams[2][6], super_hams[3][6], super_hams[4][6]])\n",
    "# to_print += 'Ave Super Classification with 8 clusters: %.4f\\n' % np.average([super_hams[0][8], super_hams[1][8], super_hams[2][8], super_hams[3][8], super_hams[4][8]])\n",
    "# to_print += 'Ave Super Classification with 10 clusters: %.4f\\n' % np.average([super_hams[0][10], super_hams[1][10], super_hams[2][10], super_hams[3][10], super_hams[4][10]])\n",
    "\n",
    "f = open('records/record_' + datasets_medium[0] + '_full_standard_super_clf.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29120882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Att291</th>\n",
       "      <th>Att292</th>\n",
       "      <th>Att293</th>\n",
       "      <th>Att294</th>\n",
       "      <th>Beach</th>\n",
       "      <th>Sunset</th>\n",
       "      <th>FallFoliage</th>\n",
       "      <th>Field</th>\n",
       "      <th>Mountain</th>\n",
       "      <th>Urban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.646467</td>\n",
       "      <td>0.666435</td>\n",
       "      <td>0.685047</td>\n",
       "      <td>0.699053</td>\n",
       "      <td>0.652746</td>\n",
       "      <td>0.407864</td>\n",
       "      <td>0.150309</td>\n",
       "      <td>0.535193</td>\n",
       "      <td>0.555689</td>\n",
       "      <td>0.580782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157332</td>\n",
       "      <td>0.247298</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770156</td>\n",
       "      <td>0.767255</td>\n",
       "      <td>0.761053</td>\n",
       "      <td>0.745630</td>\n",
       "      <td>0.742231</td>\n",
       "      <td>0.688086</td>\n",
       "      <td>0.708416</td>\n",
       "      <td>0.757351</td>\n",
       "      <td>0.760633</td>\n",
       "      <td>0.740314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251454</td>\n",
       "      <td>0.137833</td>\n",
       "      <td>0.082672</td>\n",
       "      <td>0.036320</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793984</td>\n",
       "      <td>0.772096</td>\n",
       "      <td>0.761820</td>\n",
       "      <td>0.762213</td>\n",
       "      <td>0.740569</td>\n",
       "      <td>0.734361</td>\n",
       "      <td>0.722677</td>\n",
       "      <td>0.849128</td>\n",
       "      <td>0.839607</td>\n",
       "      <td>0.812746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.112506</td>\n",
       "      <td>0.083924</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938563</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.966743</td>\n",
       "      <td>0.968649</td>\n",
       "      <td>0.869619</td>\n",
       "      <td>0.696925</td>\n",
       "      <td>0.953460</td>\n",
       "      <td>0.959631</td>\n",
       "      <td>0.966320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019267</td>\n",
       "      <td>0.031290</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>0.090959</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.512130</td>\n",
       "      <td>0.524684</td>\n",
       "      <td>0.520020</td>\n",
       "      <td>0.504467</td>\n",
       "      <td>0.471209</td>\n",
       "      <td>0.417654</td>\n",
       "      <td>0.364292</td>\n",
       "      <td>0.562266</td>\n",
       "      <td>0.588592</td>\n",
       "      <td>0.584449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198151</td>\n",
       "      <td>0.238796</td>\n",
       "      <td>0.164270</td>\n",
       "      <td>0.184290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>0.875782</td>\n",
       "      <td>0.901653</td>\n",
       "      <td>0.926227</td>\n",
       "      <td>0.721366</td>\n",
       "      <td>0.795826</td>\n",
       "      <td>0.867642</td>\n",
       "      <td>0.794125</td>\n",
       "      <td>0.899067</td>\n",
       "      <td>0.908963</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215147</td>\n",
       "      <td>0.279607</td>\n",
       "      <td>0.254413</td>\n",
       "      <td>0.134350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>0.657706</td>\n",
       "      <td>0.669877</td>\n",
       "      <td>0.692338</td>\n",
       "      <td>0.713920</td>\n",
       "      <td>0.727374</td>\n",
       "      <td>0.750354</td>\n",
       "      <td>0.684372</td>\n",
       "      <td>0.718770</td>\n",
       "      <td>0.719916</td>\n",
       "      <td>0.730645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217201</td>\n",
       "      <td>0.199491</td>\n",
       "      <td>0.048747</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>0.952281</td>\n",
       "      <td>0.944987</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.836604</td>\n",
       "      <td>0.875916</td>\n",
       "      <td>0.957034</td>\n",
       "      <td>0.953938</td>\n",
       "      <td>0.967956</td>\n",
       "      <td>0.819636</td>\n",
       "      <td>0.707311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028002</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.017547</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>0.883990</td>\n",
       "      <td>0.899004</td>\n",
       "      <td>0.901019</td>\n",
       "      <td>0.904298</td>\n",
       "      <td>0.846402</td>\n",
       "      <td>0.858145</td>\n",
       "      <td>0.851362</td>\n",
       "      <td>0.852472</td>\n",
       "      <td>0.876665</td>\n",
       "      <td>0.908187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239041</td>\n",
       "      <td>0.256158</td>\n",
       "      <td>0.226332</td>\n",
       "      <td>0.223070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>0.974915</td>\n",
       "      <td>0.866425</td>\n",
       "      <td>0.818144</td>\n",
       "      <td>0.936140</td>\n",
       "      <td>0.938583</td>\n",
       "      <td>0.935087</td>\n",
       "      <td>0.930597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.806074</td>\n",
       "      <td>0.717955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2407 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0     0.646467  0.666435  0.685047  0.699053  0.652746  0.407864  0.150309   \n",
       "1     0.770156  0.767255  0.761053  0.745630  0.742231  0.688086  0.708416   \n",
       "2     0.793984  0.772096  0.761820  0.762213  0.740569  0.734361  0.722677   \n",
       "3     0.938563  0.949260  0.955621  0.966743  0.968649  0.869619  0.696925   \n",
       "4     0.512130  0.524684  0.520020  0.504467  0.471209  0.417654  0.364292   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2402  0.875782  0.901653  0.926227  0.721366  0.795826  0.867642  0.794125   \n",
       "2403  0.657706  0.669877  0.692338  0.713920  0.727374  0.750354  0.684372   \n",
       "2404  0.952281  0.944987  0.905556  0.836604  0.875916  0.957034  0.953938   \n",
       "2405  0.883990  0.899004  0.901019  0.904298  0.846402  0.858145  0.851362   \n",
       "2406  0.974915  0.866425  0.818144  0.936140  0.938583  0.935087  0.930597   \n",
       "\n",
       "          Att8      Att9     Att10  ...    Att291    Att292    Att293  \\\n",
       "0     0.535193  0.555689  0.580782  ...  0.157332  0.247298  0.014025   \n",
       "1     0.757351  0.760633  0.740314  ...  0.251454  0.137833  0.082672   \n",
       "2     0.849128  0.839607  0.812746  ...  0.017166  0.051125  0.112506   \n",
       "3     0.953460  0.959631  0.966320  ...  0.019267  0.031290  0.049780   \n",
       "4     0.562266  0.588592  0.584449  ...  0.198151  0.238796  0.164270   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2402  0.899067  0.908963  0.895336  ...  0.215147  0.279607  0.254413   \n",
       "2403  0.718770  0.719916  0.730645  ...  0.217201  0.199491  0.048747   \n",
       "2404  0.967956  0.819636  0.707311  ...  0.028002  0.031900  0.017547   \n",
       "2405  0.852472  0.876665  0.908187  ...  0.239041  0.256158  0.226332   \n",
       "2406  1.000000  0.806074  0.717955  ...  0.073742  0.005131  0.025059   \n",
       "\n",
       "        Att294  Beach  Sunset  FallFoliage  Field  Mountain  Urban  \n",
       "0     0.029709      1       0            0      0         1      0  \n",
       "1     0.036320      1       0            0      0         0      1  \n",
       "2     0.083924      1       0            0      0         0      0  \n",
       "3     0.090959      1       0            0      0         0      0  \n",
       "4     0.184290      1       0            0      0         0      0  \n",
       "...        ...    ...     ...          ...    ...       ...    ...  \n",
       "2402  0.134350      0       0            0      0         0      1  \n",
       "2403  0.041638      0       0            0      0         0      1  \n",
       "2404  0.019734      0       0            0      0         0      1  \n",
       "2405  0.223070      0       0            0      0         0      1  \n",
       "2406  0.004033      0       0            0      0         0      1  \n",
       "\n",
       "[2407 rows x 300 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702a97f",
   "metadata": {},
   "source": [
    "Full feature standard classfification acc vs Super classification PSO selected acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42eda467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10470910472386\n",
      "Iterate  2  gbest value is  0.10166311485847494\n",
      "Iterate  3  gbest value is  0.10166311485847494\n",
      "Iterate  4  gbest value is  0.10166311485847494\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.1047325435269018\n",
      "Iterate  2  gbest value is  0.10452233554469884\n",
      "Iterate  3  gbest value is  0.10154203061304254\n",
      "Iterate  4  gbest value is  0.10154203061304254\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09831199379037225\n",
      "Iterate  2  gbest value is  0.09831199379037225\n",
      "Iterate  3  gbest value is  0.09804015334541763\n",
      "Iterate  4  gbest value is  0.09804015334541763\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.09906150874510802\n",
      "Iterate  2  gbest value is  0.09849433193971643\n",
      "Iterate  3  gbest value is  0.09849433193971643\n",
      "Iterate  4  gbest value is  0.09849433193971643\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10409872338665445\n",
      "Iterate  2  gbest value is  0.09669688336355002\n",
      "Iterate  3  gbest value is  0.09669688336355002\n",
      "Iterate  4  gbest value is  0.09669688336355002\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "#        problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "    problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_super_PSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38420eaa",
   "metadata": {},
   "source": [
    "Standard PSO FS time cost vs Super PSO FS time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9efa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10053697819836607\n",
      "Iterate  2  gbest value is  0.10053697819836607\n",
      "Iterate  3  gbest value is  0.10053697819836607\n",
      "Iterate  4  gbest value is  0.10053697819836607\n",
      "Fold  1\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10302277941047297\n",
      "Iterate  2  gbest value is  0.10302277941047297\n",
      "Iterate  3  gbest value is  0.10302277941047297\n",
      "Iterate  4  gbest value is  0.10138353021981936\n",
      "Fold  2\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.0962167647246712\n",
      "Iterate  2  gbest value is  0.0962167647246712\n",
      "Iterate  3  gbest value is  0.0962167647246712\n",
      "Iterate  4  gbest value is  0.0962167647246712\n",
      "Fold  3\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10309103367896781\n",
      "Iterate  2  gbest value is  0.10309103367896781\n",
      "Iterate  3  gbest value is  0.10309103367896781\n",
      "Iterate  4  gbest value is  0.09910224523650166\n",
      "Fold  4\n",
      "Iterate  0  gbest value is  inf\n",
      "Iterate  1  gbest value is  0.10233582798051337\n",
      "Iterate  2  gbest value is  0.10233582798051337\n",
      "Iterate  3  gbest value is  0.10233582798051337\n",
      "Iterate  4  gbest value is  0.10233582798051337\n"
     ]
    }
   ],
   "source": [
    "# Main entry\n",
    "\n",
    "data = read_arff('datasets/' + datasets[6] + '.arff')\n",
    "\n",
    "y = data.iloc[:, locations[6]:]\n",
    "X = data.iloc[:, :locations[6]]\n",
    "\n",
    "n_features = len(list(X))\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "n_splits = 5\n",
    "k_fold = IterativeStratification(n_splits=n_splits, order=1, random_state = 42)\n",
    "\n",
    "full_hams = []\n",
    "sel_hams = []\n",
    "PSO_durations = []\n",
    "f_ratios = []\n",
    "to_print = ''\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(X, y):\n",
    "    print('Fold ', fold_count)\n",
    "    fold_count += 1\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # get full acc\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf = MLkNN(k=3)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = clf.predict(X_test_scaled)\n",
    "    full_ham = hamming_loss(y_test, y_test_pred)\n",
    "    full_hams.append(full_ham)\n",
    "    \n",
    "    \n",
    "    #  perform FS\n",
    "    start_PSO = time.time_ns()    # marking start time of PSO \n",
    "    problem = FS_ML(minimize=True, X=X_train, y=y_train)\n",
    "#     problem = FS_ML_super(minimize=True, X=X_train, y=y_train)\n",
    "\n",
    "    # parameter for PSO\n",
    "    pop_size = 3\n",
    "    n_iterations = 5\n",
    "    swarm = Swarm(n_particles=pop_size, length=n_features, pos_max=1.0, pos_min=0,\n",
    "                       vel_max=0.2, vel_min=-0.2, problem=problem, n_iterations=n_iterations)    \n",
    "    best_sol, best_fit = swarm.iterate()\n",
    "    end_PSO = time.time_ns()      # marking ending time of PSO\n",
    "    duration_PSO = round((end_PSO - start_PSO)/1000000000, 2)\n",
    "    PSO_durations.append(duration_PSO)\n",
    "    \n",
    "    # process the final solution\n",
    "    sel_fea = np.where(best_sol > problem.threshold)[0]\n",
    "    clf.fit(X_train[:, sel_fea], y_train)\n",
    "    y_test_pred = clf.predict(X_test[:, sel_fea])\n",
    "    fold_ham = hamming_loss(y_true=y_test, y_pred=y_test_pred)\n",
    "    sel_hams.append(fold_ham)\n",
    "    f_ratios.append(len(sel_fea)/n_features)\n",
    "\n",
    "    # to write the results\n",
    "    to_print += '--------------Fold %d----------------\\n' % fold_count\n",
    "    to_print += 'Full feature hamming loss: %.4f\\n' % full_ham\n",
    "    to_print += 'Fold selected hamming loss: %.4f\\n' % fold_ham\n",
    "    to_print += 'Time of PSO: %.4f\\n' % duration_PSO\n",
    "    to_print += 'Selection ratio: %.2f\\n' % (len(sel_fea)/n_features)\n",
    "    to_print += 'Selected features: %s\\n' % (', '.join([str(ele) for ele in sel_fea]))\n",
    "\n",
    "to_print += '--------------Average----------------\\n'\n",
    "to_print += 'Ave Full Accuracy: %.4f\\n' % np.average(full_hams)\n",
    "to_print += 'Ave Selection Accuracy: %.4f\\n' % np.average(sel_hams)\n",
    "to_print += 'Ave time of PSO: %.4f\\n' % np.average(PSO_durations)\n",
    "to_print += 'Ave Feature Ratio: %.2f\\n' % np.average(f_ratios)\n",
    "\n",
    "f = open('records/record_' + datasets[6] + '_standardPSO.txt', 'w')\n",
    "f.write(to_print)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715c86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab51bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
